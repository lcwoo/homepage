---
title: "Learning Transferable Visual Models From Natural Language Supervision"
date: "2025-05-13"
summary: "paper review"
---

> 2021년 발표된 CLIP 논문  
> _[Radford et al., ICML 2021](https://arxiv.org/abs/2103.00020)_ 을 상세히 분석한 리뷰입니다.  
> [[paper]](https://arxiv.org/abs/2003.08934), [[blog]](https://openai.com/index/clip/), [[GitHub]](https://github.com/openai/CLIP?tab=readme-ov-file)  
> Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever  
> OpenAI    
> 2021년 1월 5일


## 목차 / Table of Contents

1. 서론: 왜 CLIP인가?
2. 관련 연구 (Related Works)
3. 데이터셋 구축 (WIT: WebImageText)
4. 모델 구조 (CLIP Architecture)
5. 학습 세부사항 (Training Details)
6. 실험 및 분석 (Experiments)
7. 한계(Limitations)
8. 결론 및 의의  

---  
 
<Image
  src="/images/paper-review/CLIP/overview.png"
  alt="clip overview"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

최근 Vision-Language 분야에서 CLIP이 등장하지 않는 논문을 찾기란 거의 불가능합니다. 
이미지-텍스트 정렬, zero-shot 분류, 멀티모달 표현 학습 등 다양한 VLM 연구들이 CLIP을 실험의 기준선으로 사용하거나 
구조적으로 변형하여 핵심 구성 요소로 채택하고 있습니다. 
여러 멀티모달 모델들을 분석하며 CLIP이 단순한 베이스라인을 넘어 이 분야의 de facto standard처럼 자리잡고 있다는 사실을 체감하게 되었습니다. 
그만큼 CLIP을 제대로 이해하지 않고는 멀티모달 학습의 구조와 발전 흐름을 정확히 파악하기 어렵다는 생각이 들었고 
이번 기회에 논문을 처음부터 끝까지 정독하고 주요 개념과 구조, 한계점까지 정리해보고자 이 리뷰를 작성하게 되었습니다. 



## 1. 서론: 왜 CLIP인가?

<HighlightText lightColor="red.300" darkColor="#ffb163"> 컴퓨터 비전 분야는 오랜 시간 동안 고정된 label에 기반한 supervised learning 방식을 중심으로 발전해왔습니다. </HighlightText>
대표적으로 ImageNet과 같은 대규모 데이터셋이 그 중심에 있었으며, 모델들은 사전에 정의된 클래스에 대해 높은 성능을 보이도록 학습되었습니다. 하지만 이러한 방식은 범용성에 제한이 있으며, 새로운 태스크나 클래스에 대한 유연한 대응이 어렵다는 한계가 존재합니다.
반면 Natural Language Processing 분야에서는 GPT와 같은 대규모 언어 모델의 등장으로 웹 규모의 데이터를 통한 범용 모델 학습이 보편화되었습니다. 이러한 모델들은 특정 태스크에 특화된 학습 없이도 다양한 문제를 풀 수 있는 능력을 보여주었습니다. 이는 라벨링된 데이터셋이 아닌 웹에 존재하는 자연어 그 자체를 supervision의 원천으로 삼는 새로운 패러다임을 열었습니다.  
<HighlightText lightColor="red.300" darkColor="#ffb163"> 이러한 맥락에서 CLIP(Contrastive Language-Image Pretraining)은 "자연어로 이미지 표현을 학습할 수 있다면 어떨까?"라는 질문에서 출발합니다. </HighlightText>  
CLIP은 이미지와 텍스트 간의 pair을 이용하여 학습된 멀티모달 모델로, 별도의 fine-tuning 없이도 다양한 down-stream 태스크에 대한 zero-shot 추론이 가능하다는 점에서 기존의 모델과 본질적으로 다른 접근 방식을 도입했습니다. 이는 단순히 하나의 모델이 아닌, 멀티모달 학습 시대의 문을 여는 전환점이 되었다고 평가받습니다.

---  

### <HighlightText fontSize="2xl">핵심 contributions</HighlightText>

<HighlightText lightColor="blue.400" darkColor="yellow.400" fontSize="xl"> 
1. 자연어 기반 Supervision (Natural Language Supervision) 
</HighlightText> 
<HighlightText lightColor="cyan.700" darkColor="#8be9fd"> &nbsp;&nbsp;CLIP은 사전 정의된 클래스 라벨 대신 웹에서 수집한 자유로운 텍스트 설명을 감독 신호로 사용합니다. 이를 통해 특정 태스크에 종속되지 않은 범용 이미지 표현 학습이 가능하며 fine-tuning 없이도 다양한 down-stream 태스크에 zero-shot으로 대응할 수 있습니다. </HighlightText>  

<HighlightText lightColor="blue.400" darkColor="yellow.400" fontSize="xl"> 
2. 대규모 이미지-텍스트 페어 데이터셋 (WIT) 
</HighlightText> 
<HighlightText lightColor="cyan.700" darkColor="#8be9fd"> &nbsp;&nbsp;기존 MS-COCO, YFCC100M의 한계를 극복하기 위해 CLIP은 인터넷에서 수집한 4억 개의 이미지-텍스트 쌍으로 구성된 WebImageText(WIT) 데이터셋을 구축합니다. 다양한 쿼리를 기반으로 데이터의 표현 범위를 넓혔으며, 이는 모델의 일반화 성능을 높이는 기반이 됩니다. </HighlightText>  

<HighlightText lightColor="blue.400" darkColor="yellow.400" fontSize="xl"> 
3. Dual Encoder 기반 Contrastive Learning 
</HighlightText> 
<HighlightText lightColor="cyan.700" darkColor="#8be9fd"> &nbsp;&nbsp;이미지와 텍스트를 각각 독립적인 인코더(ResNet/ViT, Transformer)로 임베딩한 뒤, 동일한 멀티모달 임베딩 공간에서 cosine similarity를 기반으로 학습합니다. InfoNCE loss를 사용해 정답 쌍의 유사도는 높이고, 오답 쌍은 낮추는 방식으로 훈련되며, self-supervised 방식이므로 라벨 없이도 학습이 가능합니다. </HighlightText>  

<HighlightText lightColor="blue.400" darkColor="yellow.400" fontSize="xl"> 
4. Prompt Engineering 및 Zero-Shot Inference
</HighlightText>
<HighlightText lightColor="cyan.700" darkColor="#8be9fd"> &nbsp;&nbsp;Zero-shot 분류 시, 텍스트 인코더에 “A photo of a label.”과 같은 프롬프트를 제공하여 CLIP이 적절한 의미 공간을 형성하도록 돕습니다. 다양한 템플릿과 앙상블을 통해 프롬프트 품질을 높이며, fine-tuning 없이도 다양한 태스크에 직접 활용할 수 있습니다. </HighlightText>  
  
---
  
## 2. 관련 연구 (Related Works)

<Image
  src="/images/paper-review/CLIP/relatedworks.png"
  alt="clip overview"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

#### <HighlightText lightColor="red.300" darkColor="#ffb163" fontSize="xl">기존 접근들:</HighlightText> 

<HighlightText lightColor="red.300" darkColor="#ffb163"> 1. Natural Language Supervision의 시도  </HighlightText> 

Natural Language Supervision의 시도
이미지를 자연어로 감독(supervision)하는 방식은 VirTex, ConVIRT 등의 연구에서 시도된 바 있습니다.
이들은 이미지-텍스트 쌍을 기반으로 학습된 모델이지만, 다음과 같은 공통적인 한계를 가집니다:

- 학습 가능한 클래스 수가 제한적임 (예: VirTex는 1000개, ConVIRT는 18,291개 수준)
- 모델의 학습 비용이 크고 효율성이 낮음
- zero-shot classification으로의 확장이 어려움

<HighlightText lightColor="red.300" darkColor="#ffb163">2. 데이터셋의 한계  </HighlightText> 

기존에 널리 사용된 멀티모달 데이터셋은 다음과 같습니다:

- MS-COCO, Visual Genome: 고품질이지만 수량이 적어 확장성 부족
- YFCC100M: 대규모이지만 텍스트 메타데이터가 희소하고 품질이 낮음 
> → 실제로 쓸 수 있는 image-text 쌍은 약 1,500만 개로, ImageNet과 비슷한 규모로 축소됨. 또한 ImageNet 수준의 라벨링을 수작업으로 부여할 경우, 2,200만 개의 라벨을 붙이는 데 약 25,000명의 인력이 필요하다는 추정도 있습니다. 이는 비용과 시간 면에서 매우 비효율적인 접근입니다.


<HighlightText lightColor="red.300" darkColor="#ffb163">3. Pretraining의 효율성과 컴퓨팅 자원 문제</HighlightText> 

<Image
  src="/images/paper-review/CLIP/moreefficient.png"
  alt="moreefficient"
  width={400}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

당시 SOTA CV 모델들은 다음과 같은 한계를 지녔습니다:

- ResNeXt101-32x48d: ImageNet 1000개 클래스 학습에 19 GPU-years
- EfficientNet-L2 (Noisy Student): 동일한 작업에 33 TPUv3 cores-year  
> 단 1000개 클래스만을 예측하는데도 이 정도의 컴퓨팅 자원이 필요하다는 점은, 자연어처럼 대규모 open vocabulary로 확장하기에는 큰 장벽입니다.

초기에는 VirTex처럼 이미지 CNN과 텍스트 Transformer를 결합해 caption prediction을 수행하는 방식도 시도되었으나, 이는 학습 속도가 느리고 확장성이 떨어지는 문제가 있었습니다. ResNet-50을 두 번 사용하는 Transformer 모델은 단순한 Bag-of-Words(BoW) 기반 예측 모델보다 학습 속도가 3배 이상 느린 것으로 보고되었습니다.
이러한 방식은 이미지에 연결된 텍스트를 정확히 예측하려는 접근이었으며 실제로 다양한 설명과 주석, 관련 텍스트가 존재하는 웹 이미지의 특성상 정확한 단어를 예측하는 것은 매우 어려운 작업이었습니다.  
<HighlightText lightColor="red.300" darkColor="#ffb163"> 그래서 저자들은 텍스트의 정확한 단어를 예측하기보다는 이 텍스트 전체가 이 이미지와 짝을 이루는가 만을 예측하는 contrastive objective 기반 접근을 사용합니다.  
같은 BoW baseline에서 predictive objective 대신 contrastive objective로 교체한 결과 zero-shot 전이 성능에서 4배의 효율성 향상을 관측하였습니다.  </HighlightText> 
CLIP은 하나의 배치에 있는 N개의 쌍 중 실제 짝을 맞추도록 학습됩니다. 전체 $N \times N$ 가능한 쌍 중 실제로 대응되는 쌍의 cosine similarity를 최대화하고, 나머지 $N^2-N$개의 잘못된 쌍의 유사도는 최소화하는 방식입니다. 이 과정은 대칭적인 cross-entropy loss를 기반으로 최적화됩니다.
CLIP은 기존 이미지 인코더 ImageNet pretrained나 텍스트 인코더를 사용하지 않고 처음부터 scratch로 학습되며 [Zhang et al. (2020)](https://arxiv.org/pdf/2010.00747)의 방식과 달리 non-linear projection을 제거하고 단순 linear projection 만을 사용해 멀티모달 임베딩 공간으로 변환합니다. 텍스트 및 이미지 변환 함수 역시 단순화하였고 이미지에 대해서는 랜덤 크롭만을 적용했습니다. 마지막으로 softmax의 로짓 범위를 조정하는 temperature 파라미터 $\tau$는 학습 가능한 log-scaled 스칼라로 직접 최적화됩니다.

---

## 3. 데이터셋 구축 (WIT: WebImageText)

CLIP은 기존 멀티모달 학습에 사용되던 데이터셋들의 한계를 극복하고 보다 범용적인 이미지-텍스트 표현을 학습하기 위해
약 4억 개(400M)에 달하는 웹 기반 이미지-텍스트 쌍으로 구성된 대규모 데이터셋을 새롭게 구축하였습니다.  
이 데이터셋은 WebImageText(WIT)라고 명명되었으며 인터넷 전반에 걸쳐 약 50만 개(500K)의 다양한 검색 쿼리를 기반으로 데이터를 수집하였습니다.
쿼리 bias를 줄이기 위해 하나의 쿼리당 수집 가능한 이미지 수를 최대 2만 장(20K)으로 제한하였으며 
다양한 웹 출처와 언어 표현을 포함함으로써 광범위한 도메인 커버리지를 확보하고자 하였습니다.  
이처럼 대규모이면서도 다양한 표현을 포함한 WIT 데이터셋은 CLIP이 사전 정의된 라벨 없이도 자연어를 통해 범용적인 시각 개념을 학습할 수 있도록 하는 핵심 기반입니다.
특히 웹에서 수집된 자연어 텍스트는 단순한 레이블 이상의 풍부한 context을 제공하며 CLIP은 이를 바탕으로 의미 중심의 이미지 표현을 학습하고
다양한 downstream task에 zero-shot 방식으로 전이할 수 있는 능력을 갖추게 됩니다.


---

## 4. 모델 구조 (CLIP Architecture)

CLIP은 이미지와 텍스트를 동일한 임베딩 공간상에 정렬하기 위해 각각 독립적인 인코더를 통해 두 modality를 처리한 뒤 
이들의 임베딩 간 cosine similarity를 기반으로 contrastive 학습을 수행합니다. 
이 구조는 별도의 정답 라벨 없이도 멀티모달 간 의미 정렬이 가능하도록 설계되었습니다.

### 4.1. 전체 구조 개요

이미지는 ResNet 계열 또는 Vision Transformer(ViT)를 기반으로 한 Image Encoder를 통해 처리됩니다. 
텍스트는 Transformer 기반 구조와 Byte Pair Encoding tokenizer를 사용하는 Text Encoder를 거쳐 임베딩됩니다. 
이후 두 인코더의 출력은 각각 linear projection과 L2 정규화를 거쳐 
동일한 멀티모달 임베딩 공간에서 cosine similarity를 계산할 수 있도록 조정됩니다.

### 4.2. 학습 방식

<Image
  src="/images/paper-review/CLIP/contrastive_pseudo.png"
  alt="moreefficient"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

CLIP은 InfoNCE loss를 기반으로 한 contrastive learning 방식을 채택합니다. 
하나의 미니배치에는 $N$개의 이미지-텍스트 쌍이 포함되며 각 인코더로부터 얻은 임베딩으로부터 $N \times N$의 유사도 행렬을 생성합니다. 
이때 $S_{ij}$는 이미지(i)와 텍스트(j) 사이의 cosine similarity를 의미합니다. 

$$
S_{ij} = \text{cosine}(f_i^{\text{img}}, f_j^{\text{text}})
$$

이 유사도 행렬은 symmetric cross-entropy loss를 통해 최적화되며 
이미지 → 텍스트 텍스트 → 이미지 두 방향의 매칭을 동시에 학습합니다. 

$$
\mathcal{L}_{\text{image} \rightarrow \text{text}} = \frac{1}{N} \sum_{i=1}^{N} -\log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{N} \exp(S_{ij}/\tau)}
$$
$$
\mathcal{L}_{\text{text} \rightarrow \text{image}} = \frac{1}{N} \sum_{i=1}^{N} -\log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{N} \exp(S_{ji}/\tau)}
$$
$$
\mathcal{L} = \frac{1}{2}(\mathcal{L}_{\text{image} \rightarrow \text{text}} + \mathcal{L}_{\text{text} \rightarrow \text{image}})
$$

최종 손실은 이 두 방향의 평균으로 계산됩니다.
이 구조는 정답 라벨 없이도 학습이 가능하므로 self-supervised learning 방식으로 분류됩니다. 
특히 학습 도중 temperature 파라미터 는 학습 가능한 log-scaled 스칼라로 설정되어 softmax의 sharpness를 조절하며 안정적인 학습을 가능하게 합니다. 
$\tau = \frac{1}{exp(s)}$ 형태로 parameterize되어 항상 양수 값을 유지하며 지수 함수로 변화함으로써 gradient explosion이나 vanishing을 방지합니다.

### 4.3. Prompt Engineering

<Image
  src="/images/paper-review/CLIP/prompt_engineering.png"
  alt="moreefficient"
  width={400}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

CLIP은 단순히 클래스 이름만을 텍스트 인코더에 입력하는 대신, "A photo of a \{label\}."과 같은 자연어 문장 템플릿을 사용합니다. 
이러한 방식은 단어에 문맥을 부여하여 텍스트 인코더가 더 풍부하고 정밀한 의미 표현을 학습할 수 있도록 도와줍니다. 
예를 들어 "crane"처럼 의미가 여러 개인 단어의 경우 문장 구조 내 문맥을 통해 의미 모호성을 효과적으로 해소할 수 있습니다.  
&nbsp;&nbsp;또한 CLIP은 prompt ensembling 기법을 도입하여 하나의 클래스에 대해 다양한 문장 템플릿을 적용한 후 
각각의 텍스트 임베딩을 생성하고 이를 평균내어 하나의 대표 임베딩으로 사용합니다. 
"A photo of a big \{label\}.", "A photo of a small \{label\}." 등 
서로 다른 프롬프트를 통해 생성된 임베딩들을 임베딩 공간에서 평균내는 방식입니다.
이 방식은 softmax 확률의 평균이 아닌 임베딩 자체의 평균을 사용하는 점에서 일반적인 앙상블과 구분됩니다. 
덕분에 평균 임베딩을 미리 캐싱해두면 추론 시 여러 프롬프트를 사용하는 것과 유사한 성능을 내면서도 계산 비용은 거의 증가하지 않습니다. 
CLIP 논문에서는 ImageNet 실험에서 무려 80개의 프롬프트를 앙상블함으로써 단일 프롬프트 대비 3.5%의 정확도 향상을 얻었고 
프롬프트 엔지니어링과 앙상블 전략을 결합하면 최대 5%에 가까운 성능 향상이 가능하다고 보고했습니다. 

---

## 5. 학습 세부사항 (Training Details)

CLIP은 대규모 데이터와 모델을 학습하기 위해 다음과 같은 최적화 기법과 세부 전략을 사용했습니다:

- Optimizer: AdamW (decoupled weight decay regularization)
- Learning Schedule: Cosine decay schedule을 사용하여 학습률을 점진적으로 감소
- Batch Size: 32,768로 매우 큰 미니배치를 사용하여 안정적인 학습 진행
- Mixed Precision Training: 학습 속도를 높이고 GPU 메모리를 절약하기 위해 FP16을 사용
- Gradient Checkpointing: 메모리 절약을 위한 기법으로 중간 계산 값을 저장하지 않고 역전파 시 재계산함
- Temperature Parameter : contrastive loss에서 softmax 분포의 날카로움을 조절하는 파라미터로 학습 가능한 변수로 설정됨

ViT-L/14 아키텍처를 사용하고 입력 해상도를 336px로 설정한 모델(ViT-L/14@336px)이 가장 높은 성능을 보여주었으며 
이는 논문 내 실험에서 CLIP의 대표 모델로 사용됩니다.
학습에 사용된 컴퓨팅 리소스는 GPU 수백 장이 10일 이상 소요될 정도로 매우 큰 규모였으며 
이러한 리소스를 통해 모델은 32 epoch 동안 전체 4억 개의 데이터셋을 학습합니다.

---

## 6. 실험 및 분석 (Experiments and Analysis)

CLIP은 별도의 fine-tuning 없이도 다양한 벤치마크에서 강력한 zero-shot 성능을 보여줍니다. 
기존 ImageNet에 특화된 supervised 모델들과 달리 CLIP은 다양한 도메인의 데이터셋으로의 distribution shift 상황에서도 
보다 강건한 성능을 나타냅니다.

<Image
  src="/images/paper-review/CLIP/robustness_gap.png"
  alt="clip robustness"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

위 그림에서 왼쪽은 ImageNet 기반 모델과 다양한 자연 이미지 데이터셋 간의 성능 차이를 나타냅니다. 
이상적인 강건한 모델은 모든 분포에서 동일한 성능을 보여야 하지만 기존 모델들은 성능이 크게 저하됩니다. 
반면 CLIP은 이러한 robustness gap을 최대 75%까지 줄이는 성과를 보입니다.
오른쪽 시각화는 'banana' 클래스에 대해 분포가 다른 5개 데이터셋에서의 예시를 보여줍니다. 
동일한 ImageNet 정확도를 가진 ResNet-101과 CLIP ViT-L/14@336px 모델을 비교한 결과 
CLIP이 훨씬 더 다양한 분포에 잘 적응하는 모습을 확인할 수 있습니다.

---

## 7. 한계 (Limitations)

<HighlightText lightColor="red.300" darkColor="#ffb163" fontSize="xl"> CLIP은 강력한 zero-shot 성능과 높은 범용성을 보여주지만 여전히 여러 가지 중요한 한계를 갖고 있습니다.  </HighlightText>  
<HighlightText lightColor="red.300" darkColor="#ffb163">CLIP은 fine-grained classification 태스크에서 성능이 떨어지는  경향을 보입니다.</HighlightText>  
 자동차 모델, 꽃의 종, 항공기 변형 등 세부적인 시각적 차이를 구분해야 하는 과제에서는 task-specific 모델에 비해 정확도가 낮았습니다.
또한 사물의 개수를 세거나 추상적 개념을 이해하는 등 체계적인 reasoning이 요구되는 과제에서도 약한 성능을 보였습니다.
CLIP은 OCR에는 높은 정확도를 보이지만 손으로 쓴 숫자를 인식하는 MNIST에서는 단순한 logistic regression보다도 낮은 성능을 기록했습니다. 
이는 CLIP의 사전학습 데이터에 해당 분포가 포함되지 않았기 때문이며 CLIP이 실제로는 여전히 데이터 분포에 매우 민감하다는 점을 시사합니다.  

<HighlightText lightColor="red.300" darkColor="#ffb163"> CLIP의 학습 데이터는 인터넷에서 수집된 필터링되지 않은 이미지-텍스트 쌍으로 구성되어 있어 social bias을 그대로 학습할 수 있는 위험도 존재합니다. </HighlightText> 
이는 기존의 이미지 캡셔닝 모델에서도 관찰된 문제이며 CLIP 또한 성별, 인종, 직업 등과 관련된 편향을 내포할 가능성이 큽니다. 
또한 CLIP은 zero-shot 분류에는 강하지만 few-shot setting에서는 오히려 성능이 하락하는 현상도 보고되었습니다. 
일반적으로 인간은 적은 수의 예시만으로도 학습이 가능하지만 CLIP은 학습 없이 바로 분류하는 zero-shot 상황에서는 강점을 가지면서도 
소량의 labeled 데이터를 제공할 경우 성능이 비약적으로 향상되지 않는 한계를 드러냅니다. 
이는 linear probing만으로는 CLIP의 표현력을 충분히 활용하지 못하기 때문으로 분석됩니다.   
<HighlightText lightColor="red.300" darkColor="#ffb163"> 계산 자원과 관련해서도 중요한 제약이 존재합니다.  </HighlightText> 
논문에서는 zero-shot CLIP이 state-of-the-art 성능에 도달하기 위해서는 약 1000배의 연산량 증가가 필요하다고 추정하며 
이는 현재의 하드웨어 환경에서는 실질적으로 불가능한 수준입니다. 
CLIP은 본질적으로 매우 비효율적인 학습 구조를 갖고 있으며 대규모 데이터와 compute에 의존하고 있습니다. 
따라서 앞으로는 CLIP의 학습 효율성을 개선하기 위한 연구가 필요합니다.  
<HighlightText lightColor="red.300" darkColor="#ffb163"> 마지막으로 CLIP의 평가 방식에도 제약이 있습니다.  </HighlightText> 
논문에서 사용된 여러 벤치마크는 CLIP 모델 개발 과정과 밀접하게 연관되어 있으며 
완전히 새로운 태스크에서의 일반화를 충분히 검증하지 못합니다. 
또한 zero-shot setting이라고 하면서도 모델 개발 중 수차례 validation set을 확인한 점은 
진정한 의미의 zero-shot 실험이라 보기 어렵다는 한계도 존재합니다. 
따라서 향후에는 보다 공정하고 일관된 zero-shot 벤치마크가 요구됩니다.

---

## 8. 결론 및 의의

CLIP은 자연어를 supervision 신호로 활용하여 이미지를 학습하는 새로운 학습 방식을 제안함으로써 
다양한 다운스트림 태스크에 대해 사전 학습된 모델만으로도 zero-shot 추론이 가능한 범용 멀티모달 모델을 실현하였습니다. 
기존의 이미지 분류 모델들은 고정된 클래스에 한정된 학습을 수행했기 때문에 새로운 태스크나 도메인에 대해 일반화하기 어려웠습니다. 
CLIP은 이러한 한계를 극복하고자 대규모 이미지-텍스트 쌍을 기반으로 텍스트와 이미지를 각각 독립된 인코더에서 임베딩한 후 
이들 간의 의미적 정렬을 contrastive learning 방식으로 학습하였습니다.
이러한 구조 덕분에 CLIP은 이미지-텍스트 검색이나 분류와 같은 태스크에 대해 별도의 추가 학습 없이도 뛰어난 성능을 보여주었고 
텍스트와 이미지를 동일한 표현 공간 상에서 비교 가능한 형태로 만들어 멀티모달 표현 학습의 새로운 가능성을 열었습니다. 
이는 전통적인 고정 클래스 기반의 학습 방식에서 벗어나 자연어라는 보다 유연하고 표현력이 풍부한 입력을 통해 
모델이 다양한 과제를 이해하고 수행할 수 있도록 한 점에서 커다란 패러다임 전환으로 평가됩니다.