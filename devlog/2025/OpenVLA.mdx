---
title: "OpenVLA: An Open-Source Vision-Language-Action Model"
date: "2025-05-17"
summary: "paper review"
---

> 2024년 발표된 OpenVLA 논문  
> [[paper]](https://arxiv.org/pdf/2406.09246), [[GitHub]](https://github.com/openvla/openvla)  
> Moo Jin Kim*, Karl Pertsch*, Siddharth Karamcheti*, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn *Equal contribution  
> Stanford University, UC Berkeley, Toyota Research Institute, Google DeepMind, Physical Intelligence, MIT  
> CoRL 2024  
> 2024년 6월 13일  


#### This review is still a work in progress. The full post will be published soon.

<Image
  src="/images/paper-review/OpenVLA/openvla.png"
  alt="GroundingDINO overview"
  width={600}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

## 목차 / Table of Contents

1. 서론 (Introduction)
2. 주요 기여 (Contributions)
3. 관련 연구 (Related Work)
4. 모델 아키텍처 (Model Architecture)
5. 학습 방법론 (Training Methodology)
6. 코드베이스 구성 (OpenVLA Codebase)
7. 실험 분석 (Experiments)
8. 논의 및 한계점 (Discussion & Limitations)
9. 결론 (Conclusion)