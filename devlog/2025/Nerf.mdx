---
title: "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
date: "2025-05-04"
summary: "paper review"
---

> 2020년 발표된 NeRF 논문  
> _[Mildenhall et al., ECCV 2020](https://arxiv.org/abs/2003.08934)_ 을 상세히 분석한 리뷰입니다.  
> [[paper]](https://arxiv.org/abs/2003.08934), [[project]](https://www.matthewtancik.com/nerf), [[GitHub]](https://github.com/bmild/nerf)  
> Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng  
> UC Berkeley | Google Research | UC San Diego   
> 2020년 3월 19일


## 목차 / Table of Contents

1. [서론 / Introduction](#1)
2. [전체 파이프라인 / Full Pipeline](#2)
3. [구현 세부절차 / Implementation Details](#3)
4. [볼륨 렌더링 / Volume Rendering](#4)
5. [MLP 입력 구조 / MLP Input Structure](#5)
6. [Hierarchical Sampling](#6)
7. [손실 함수 / Loss Function](#7)
8. [실험 및 결과 / Experiments and Results](#8)
9. [결론 / Conclusion](#9)


---
<Image
  src="/images/paper-review/NeRF/overview-nerf.png"
  alt="NeRF Overview"
  width={800}
  height={450}
/>


&nbsp;&nbsp;최근 진행한 로봇 프로젝트를 통해, 로봇이 물체를 집거나 조작하는 과정에서 정확한 3차원 장면 복원 없이는 정밀한 제어가 어렵다는 점을 실감했습니다. 단순히 대상의 위치나 윤곽만 아는 것에 그치지 않고, 형상, 깊이, 반사, 투명도 등 복합적인 시각 정보를 정밀하게 복원할 수 있어야 실제 환경에서 안정적인 인지가 가능하다는 사실을 경험했습니다. 이러한 문제의식 속에서, 복잡한 장면을 연속적인 고차원 표현으로 정밀하게 모델링하는 기술들에 관심을 갖게 되었고 Neural Radiance Fields (NeRF)라는 접근 방식을 접하게 되었습니다. 처음에는 하나의 기법으로 등장했지만, 지금은 NeRF 자체가 3D 신경 표현 분야 전체의 출발점이자 기준점으로 자리잡았다고 생각합니다.  
&nbsp;&nbsp;이번 글에서는 NeRF를 처음 제안한 논문을 중심으로 그 핵심 아이디어와 수식 구조를 설명하고, 이어서 이 아이디어가 어떻게 발전되어 왔는지 주요 후속 연구들을 함께 정리해보려 합니다. 즉, 기초 개념 정리부터 기술 흐름까지 정리해보고자 합니다.

---

## 1. Introduction

&nbsp;&nbsp;NeRF(Neural Radiance Fields)가 처음 제안된 목적은 Novel View Synthesis, 즉 주어진 시점에서 촬영된 이미지들만을 가지고 보지 못한 새로운 시점의 이미지를 rendering하는 것입니다. 이 문제는 컴퓨터 비전과 그래픽스 분야에서 오랫동안 연구되어온 과제로, sparse한 이미지 관측만으로도 연속적인 시점 변화에 대응하는 realistic한 결과를 생성하는 것이 핵심입니다.  
기존에는 view interpolation이나 mesh, voxel, point cloud 기반 방식들이 사용되었지만, 이들은 막대한 메모리와 계산 비용, 시점 변화에 따른 표현력 부족이라는 근본적인 한계를 지니고 있었습니다.  
&nbsp;&nbsp;NeRF는 이러한 문제를 해결하기 위해 장면을 연속적인 5차원 함수로 모델링합니다. 
<HighlightText lightColor="red.300" darkColor="#ffb163" fontSize="xl">
3D 공간상의 위치 $(x, y, z)$와 시점 방향 $(\theta, \phi)$를 입력으로 받아, 해당 지점의 밀도 $\sigma$와 색상 $c(x,d)$를 출력하는 MLP를 학습합니다. 이 표현은 단순한 RGB 이미지와 카메라 포즈 정보만을 입력으로 받아, 각 픽셀을 관통하는 ray를 따라 샘플링된 point들의 color와 density를 예측하고, 이를 미분 가능한 volume rendering으로 누적하여 실사에 가까운 novel view를 합성합니다.
</HighlightText>
 이러한 접근 방식은 discrete한 입력(view image들)만 가지고도 연속적인 시점(view direction)에 대응하는 결과를 만들어낼 수 있다는 점에서, continuous scene representation이라는 새로운 패러다임을 열었고, 이후 다양한 후속 연구의 출발점이 되었습니다.

---

###  NeRF의 핵심 구성 요소

<HighlightText lightColor="red.300" darkColor="yellow.400" fontSize="xl">
1. 연속적인 5D 표현
</HighlightText>
&nbsp;&nbsp;NeRF는 3D 위치 $(x,y,z)$와 시점 방향 $(θ,ϕ)$로 이루어진 5D 좌표를 입력으로 받아,위치에 따른 volume density $\sigma$, view-dependent radiance $c(x,d)$를 출력하는 MLP를 학습합니다.

<HighlightText lightColor="#ff6b6b" darkColor="yellow.400" fontSize="xl">
2. 미분 가능한 볼륨 렌더링
</HighlightText>
&nbsp;&nbsp;볼륨 렌더링은 광선에 따라 샘플링된 위치의 색상 및 밀도를 누적하여 최종 픽셀 색상을 계산합니다. 이 과정은 미분 가능하여, 렌더링된 이미지와 실제 이미지 간의 오차를 최소화하는 방식으로 MLP를 학습할 수 있습니다. 또한, 연산 효율성과 정밀도를 동시에 확보하기 위해, coarse → fine 구조의 계층적 샘플링(Hierarchical Sampling) 기법을 적용합니다.

<HighlightText lightColor="#ff6b6b" darkColor="yellow.400" fontSize="xl">
3. 고주파 학습을 위한 Positional Encoding
</HighlightText>
&nbsp;&nbsp;MLP는 고주파 신호(세밀한 경계, 미세 구조 등)를 학습하기 어렵다는 문제가 있습니다. 이를 극복하기 위해 NeRF는 입력 5D 좌표에 사인·코사인 기반 Positional Encoding을 적용하여 고차원으로 임베딩합니다.

---

### <HighlightText fontSize="2xl">핵심 정리 (요약)</HighlightText>

- <HighlightText lightColor="cyan.700" darkColor="#8be9fd">5D MLP 기반 연속 장면 표현</HighlightText>
- <HighlightText lightColor="cyan.700" darkColor="#8be9fd">미분 가능한 볼륨 렌더링으로 이미지와 포즈만으로 학습 가능</HighlightText>
- <HighlightText lightColor="cyan.700" darkColor="#8be9fd">Positional Encoding을 통해 고주파 세부 표현 학습 가능</HighlightText>
- <HighlightText lightColor="cyan.700" darkColor="#8be9fd">Hierarchical Sampling으로 효율적인 학습 구조 구현</HighlightText>

###
---

## 2. NeRF의 동작 메커니즘: 전체 파이프라인 분석

<Image src="/images/paper-review/NeRF/pipeline.jpg" alt="NeRF pipeline" width={800} height={450} />
>  &nbsp;&nbsp;NeRF의 입력부터 출력까지의 전체 파이프라인 과정을 시각적으로 요약한 것이다.

### (a) 5D Input (Position + Direction)
&nbsp;&nbsp;카메라 픽셀을 기준으로, rays를 따라 여러 위치 $(x,y,z)$와 를 샘플링하고, 시점 방향 $(\theta,\phi)$와 결합해5D 좌표로 구성합니다. 이는 MLP $F_\Theta$의 입력이 됩니다. 예를 들어, NeRF 논문에서 사용된 Blender Synthetic Dataset(예: Lego, Drums 등)은 기본 해상도 800×800으로 구성되어 있지만, 실험에서는 --half_res 설정을 통해 400×400 해상도로 줄여 사용하며, 이 경우 각 이미지에서 160,000개의 ray가 생성됩니다. 학습 시에는 이 중 4096개의 ray를 무작위로 샘플링합니다.  
&nbsp;&nbsp;각 ray에 대해 먼저 64개의 위치를 stratified sampling으로 선택하여 coarse 네트워크에 전달하고, 여기서 얻은 밀도 값을 기반으로 확률 밀도 함수를 구성하여 중요도 기반으로 128개의 위치를 추가 샘플링하여 fine 네트워크에 입력합니다. 따라서 각 ray는 최대 192개의 위치에서 예측이 수행됩니다.  
&nbsp;&nbsp;이때 ray의 origin과 direction은 COLMAP과 같은 SfM 도구를 통해 복원한 카메라의 extrinsic matrix (camera-to-world) 및 intrinsic parameters(focal length, principal point 등)를 통해 계산되며, 일반적으로 이 pose 정보는 transforms.json 파일로 저장됩니다.

### (b) Output: Color + Density
&nbsp;&nbsp;NeRF의 MLP는 입력된 위치 및 방향 정보를 바탕으로, 해당 지점의 volume density와 view-dependent RGB color을 출력합니다. 이는 장면의 광학적 특성—예를 들어, 불투명한 물체의 외곽선이나 반사 특성 등—을 정밀하게 표현하는 데 핵심적인 역할을 합니다.  
&nbsp;&nbsp;밀도 $\sigma$는 위치 $(x,y,z)$에만 의존하며, 광선이 해당 위치를 통과할 때 얼마나 많은 색상 정보가 누적되는지를 나타냅니다. 이 값은 장면의 geometry를 나타내는 요소입니다.  
&nbsp;&nbsp;&nbsp;&nbsp;색상 $\mathbf{c}=(r,g,b)$는 위치뿐 아니라 시점 방향 $(\theta, \phi)$에도 의존합니다. 이로 인해 NeRF는 같은 위치라도 시점에 따라 색상이 달라지는 비-Lambertian 재질까지 표현할 수 있습니다.  
NeRF는 이 두 출력을 하나의 다층 퍼셉트론(MLP)을 통해 예측합니다. 먼저, 3D 위치 정보는 Positional Encoding을 통해 고차원 임베딩된 후 MLP에 입력되고, 이를 바탕으로 밀도 $\sigma$와 중간 feature를 출력합니다. 이후 이 중간 feature와 시점 방향 정보를 결합하여 최종 RGB 색상을 출력합니다.

### (c) Volume Rendering
  NeRF는 단일 지점의 색상과 밀도를 예측하는 것에 그치지 않고, 카메라에서 쏜 하나의 ray가 장면을 통과하며 얻는 누적 색상을 계산해야 합니다. 이를 위해 고전적인 볼륨 렌더링 기법을 사용합니다.

  하나의 ray는 장면을 따라 여러 지점을 샘플링하며, 각 지점에서 예측된 밀도와 색상이 최종 이미지의 한 픽셀에 기여하게 됩니다. 이때 각 샘플의 기여도는 해당 위치의 투명도와 색상 정보, 그리고 투과율에 따라 결정됩니다.

  이러한 누적 계산은 연속적인 적분을 이산 샘플로 근사하여 처리하며, 전체 과정은 미분 가능하기 때문에 신경망 학습에 사용할 수 있습니다.

### (d) Rendering Loss
렌더링된 색상 $\hat{C}(r)$과 ground truth 간의 $L_2$ loss를 최소화하며 MLP 파라미터 $\Theta$를 학습합니다.
$$
\mathcal{L} = \sum_r \| \hat{C}(r) - C^*(r) \|^2
$$
즉, 카메라 포즈와 함께 주어진 실제 이미지에서 각 픽셀에 대응되는 ray를 따라 볼륨 렌더링을 수행한 후 예측된 픽셀 색상 $\hat{C}(r)$과 실제 색상 $C^*(r)$ 간의 차이를 계산하고, 이 오차가 작아지도록 MLP를 학습하는 방식입니다.

  또한 NeRF는 단일 네트워크만 사용하는 것이 아니라, coarse network와 fine network 두 단계의 MLP를 사용하여, 먼저 전체 영역을 대략적으로 예측한 뒤 중요한 구간을 더 정밀하게 샘플링합니다. 이로써 학습 효율성과 표현 정밀도를 동시에 달성할 수 있습니다.

---

## 3. NeRF 구현 세부 절차

####  COLMAP을 통한 카메라 파라미터 복원

<Image src="/images/paper-review/NeRF/image_projection.png" width={800} height={450} alt="..." />

NeRF 구현은 각 이미지에 대한 카메라 포즈와 내부 파라미터 복원에서 시작되며, 이를 위해 Structure-from-Motion(SfM) 기반의 COLMAP을 사용합니다. 각 이미지 간의 feature 매칭 및 bundle adjustment를 통해 회전 행렬 \( R \), 이동 벡터 \( t \), 내부 파라미터 \((f_x, f_y, c_x, c_y)\)를 추정합니다.
COLMAP의 포즈는 world-to-camera 형식이며, 이를 camera-to-world로 변환해서 ray의 origin을 구할 수 있습니다. 

$$
\mathbf{o} = - R^{\top} \cdot t
$$

ray 방향을 계산하기 위해서는 픽셀 좌표 \((u,v)\)를 카메라 내부 파라미터를 기반으로 정규화해야 하며 아래 식처럼 표현이 됩니다.

$$
\left( \frac{u - c_x}{f_x}, \frac{v - c_y}{f_y}, 1 \right)
$$

이후 $R_{\text{cw}}$로 world 좌표계 방향으로 변환하고, 단위 벡터로 정규화하여 사용합니다.  
이러한 정보는 모두 `transforms_train.json`에 저장되며, 학습 시 참조됩니다. COLMAP이 생성한 sparse point cloud는 학습에는 사용되지 않지만, 시각화 및 scene bounds 설정에 참고됩니다.

---

####  Ray 정의 및 Stratified Sampling
NeRF에서의 핵심은 2차원 이미지의 각 픽셀로부터 3차원 공간상의 연속적인 지점을 따라 샘플링하고 각 지점들에서 색상(RGB)과 밀도(σ)를 예측하여 다시 2차원 이미지로 투영하는 것입니다. 이 과정을 위해 NeRF는 먼저 각 픽셀마다 하나의 광선(ray) 을 정의합니다. 이 ray는 장면을 관통하며 여러 지점에서 MLP를 통해 장면 정보를 예측하는 기준선 역할을 합니다.
<div style={{ textAlign: 'center' }}>
  <Image
    src="/images/paper-review/NeRF/ray.png"
    alt="NeRF pipeline"
    width={400}
    height={450}
    style={{ display: 'block', margin: '0 auto' }}
  />
</div>

각 ray는 다음과 같은 수식으로 정의됩니다:

$$
r(t) = \mathbf{o} + t ⋅ \mathbf{d}, \quad t \in [t_n, t_f]
$$

- $\mathbf{o}$: 카메라 위치 (COLMAP에서 추출한 camera-to-world matrix에서 translation 성분)
- $\mathbf{d}$: 픽셀을 기준으로 생성된 방향 벡터 (카메라 intrinsic matrix를 통해 계산된 후, world 좌표계로 변환됨)
- $t$: 광선 상에서의 샘플 위치를 지정하는 스칼라 값

이때 t는 ray 상의 샘플링 지점을 의미하며, NeRF는 여러 $t_i$ 값을 따라 $r(t_i)$ 위치에서 MLP를 통해 색상과 밀도를 예측합니다. 하지만 모든 이미지의 모든 픽셀에 대해 ray를 생성하고 계산한다면 학습 시간과 메모리 사용량이 과도하게 커지므로, NeRF는 매 iteration마다 일부 이미지와 일부 픽셀만 무작위로 선택하여 ray를 생성하고 학습에 사용합니다. 이를 통해 효율적인 학습이 가능합니다.

<div style={{ textAlign: 'center' }}>
  <Image
    src="/images/paper-review/NeRF/stratified_sampling.png"
    alt="NeRF pipeline"
    width={400}
    height={450}
    style={{ display: 'block', margin: '0 auto' }}
  />
</div>

> 각 ray는 near–far 구간 $[t_n, t_f]$에서 샘플링되며, 이 범위 내에서 여러 지점에서의 색상과 밀도를 예측합니다


#### Stratified Sampling:

Ray가 정의되면 그 위를 따라 여러 지점을 샘플링해야 합니다. 이때 단순히 $[t_n, t_f]$ 구간에서 균등 간격으로 샘플링하면 특정 패턴이 반복되어 overfitting 또는 aliasing이 발생할 수 있습니다. 이를 방지하고 샘플 분포의 다양성을 확보하기 위해 NeRF는 Stratified Sampling, 즉 계층적 무작위 샘플링 기법을 사용합니다.

이 기법은 다음과 같은 방식으로 작동합니다:
- 전체 샘플링 구간 $[t_n,t_f]$을 $N$개의 동일한 폭을 가진 구간으로 나눈다.
- 각 구간마다, 그 내부에서 하나의 위치 $t_i$를 무작위로 선택한다.

수식으로 표현하면 다음과 같다:

$$
t_i \sim \mathcal{U} \left[ t_n + \frac{i - 1}{N}(t_f - t_n),\; t_n + \frac{i}{N}(t_f - t_n) \right]
$$
<HighlightText fontSize="lg" lightColor="red.400" darkColor="#ffb163">
이 방식은 전체 구간을 고르게 커버하면서도 각 구간 안에서 무작위성을 도입해 bias과 variance을 동시에 제어해 NeRF는 더 안정적이고 일반화 가능한 학습을 수행할 수 있습니다.
</HighlightText>
Stratified Sampling은 수치적 통합(numerical integration) 기법 중 하나인 Monte Carlo Integration의 variance 감소 전략과 동일한 수학적 효과를 가지며, NeRF의 학습 수렴 속도와 품질에 크게 기여합니다. 특히 정적인 카메라 시점에서도 overfitting 없이 전체 장면을 잘 일반화할 수 있도록 돕는 중요한 역할을 합니다.

---

### 4. 볼륨 렌더링과 색상 예측

NeRF의 MLP는 이 ray가 지나가는 각 위치 $r(t)$에 대해 두 가지 물리량을 예측합니다. 하나는 그 지점에 물체가 존재할 확률을 나타내는 밀도 $\sigma(t)$, 그리고 다른 하나는 그 위치에서 주어진 방향 $\mathbf{d}$로 관찰될 때의 색상 $c(t, \mathbf{d})$입니다.
이처럼 얻어진 예측값들을 바탕으로, NeRF는 각 픽셀의 최종 색상 $C(r)$를 다음과 같은 연속 적분 형태로 정의합니다.

$$
C(r) = \int_{t_n}^{t_f} T(t) \, \sigma(t) \, c(t, d) \, dt
$$ 

여기서 $T(t)$는 누적 투과율(transmittance)을 의미하는 함수로, ray가 시작점 $t_n$부터 어떤 지점 $t$까지 흡수되지 않고 살아남을 확률을 나타냅니다. 이 함수는 누적된 밀도를 기반으로 아래와 같이 표현됩니다.

$$
T(t) = \exp\left( -\int_{t_n}^t \sigma(s) \, ds \right)
$$

이 수식은 직관적으로 해석할 수 있습니다. 
만약 ray가 지나온 구간에 높은 밀도(즉, 물체가 많이 존재하는 영역)가 많았다면, $T(t)$는 작아지고 해당 지점의 색상은 최종 픽셀 색상에 거의 기여하지 못하게 됩니다. 반면, ray가 거의 흡수되지 않고 특정 지점 $t$까지 도달할 수 있었다면 $T(t)$는 커지며, 이 지점의 색상이 더 큰 영향력을 가지게 됩니다.
NeRF의 이 볼륨 렌더링 수식은 물리적 모델링 관점 외에도, 확률적 기대값의 형태로 해석할 수 있다는 점에서 매우 흥미롭습니다. 
<HighlightText fontSize="lg" lightColor="red.400" darkColor="#ffb163">
$\sigma(t)$를 위치 $t$에 물체가 존재할 확률 밀도, $T(t)$를 해당 위치까지 ray가 살아남을 생존 확률, 그리고 $c(t, \mathbf{d})$를 조건부 색상
</HighlightText>
이라고 해석한다면, 전체 적분은 다음과 같은 확률 가중 평균 형태로 볼 수 있습니다:

$$
C(r) = \mathbb{E}_{t \sim p(t)}[c(t)] = \int p(t) \cdot c(t) \, dt, \quad \text{where } p(t) = T(t) \cdot \sigma(t)
$$

$p(t)$는 ray 상의 각 지점 $t$에서 색상이 관측될 확률 밀도 함수입니다. 따라서 NeRF가 수행하는 색상 예측은 물리적 누적이 아니라 확률 분포 상에서의 기대값 계산 즉 확률적으로 가중된 평균 색상을 추정하는 것이라고 볼 수 있습니다.

이러한 연속 적분은 실제로 계산하는 것이 불가능하기 때문에, NeRF는 이 과정을 discretization하여 근사합니다. ray 구간 $[t_n, t_f]$ 상에 $N$개의 샘플 지점을 배치하고, 각 샘플 간격을 $\delta_i$라고 했을 때, 최종 색상은 다음과 같이 근사됩니다.

$$
C(r) \approx \sum_{i=1}^{N} w_i \cdot c_i, \quad
w_i = T_i \cdot \alpha_i, \quad
\alpha_i = 1 - \exp(-\sigma_i \cdot \delta_i)
$$

$\alpha_i$는 샘플 구간 $\delta_i$ 내에서 해당 위치의 흡수율(또는 발생 확률)이며, $\sigma_i$가 클수록 $\alpha_i$도 커집니다. 특히 $\sigma_i \delta_i$가 매우 작을 경우, 다음과 같은 근사식을 적용할 수 있습니다:

$$
\exp(-\sigma_i \delta_i) \approx 1 - \sigma_i \delta_i
\quad \Rightarrow \quad
\alpha_i \approx \sigma_i \cdot \delta_i
$$

따라서 최종적으로 각 지점의 가중치 $w_i$는 다음과 같은 Riemann sum 형태로 다시 표현할 수 있습니다:

$$
w_i \approx T_i \cdot \sigma_i \cdot \delta_i
$$

결국 NeRF의 연속 적분은 이렇게 이산화된 형태로 자연스럽게 근사되며, 이는 학습 가능한 렌더링 파이프라인과 완전히 정합되는 구조입니다. 각 샘플의 색상은 확률적으로 가중되어 누적되며, 전체 ray를 따라 관찰되는 색상 분포는 확률 기반의 기대값으로 추정됩니다.

---

### 5. MLP 입력 및 구조

NeRF는 3차원 장면 내 임의의 지점에서 ray를 따라 관찰되는 색상과 밀도 값을 예측하기 위해 다층 퍼셉트론(MLP)을 사용합니다. 그러나 단순한 위치 벡터 $(x,y,z)$와 시점 방향 벡터 $(\theta,\phi)$만을 저차원 형태로 입력하면, MLP는 경계, 텍스처, 반사와 같은 고주파 정보를 제대로 표현하지 못하는 한계를 가집니다. 이는 Rahaman et al.(2019)의 연구 [[On the Spectral Bias of Neural Networks]](https://arxiv.org/abs/1806.08734)에서도 지적되었듯, 
<HighlightText fontSize="lg" lightColor="red.400" darkColor="#ffb163">
일반적인 신경망이 낮은 주파수부터 우선 학습하려는 spectral bias을 가지기 때문입니다.
</HighlightText>
 NeRF는 이러한 한계를 극복하기 위해 Positional Encoding을 진행합니다. 본 섹션에서는 그 중 전자인 Positional Encoding과 MLP의 구조적 설계에 대해 자세히 설명합니다. NeRF는 각 ray 상의 샘플링 지점에 대해, 위치 벡터 
$x$와 viewing direction 벡터 $d$를 입력으로 받아 밀도와 색상RGB을 예측합니다. 그러나 이 두 벡터는 단순한 3차원 좌표로는 표현력이 부족하기 때문에, NeRF는 이들을 고차원 공간으로 사인·코사인 변환하는 Positional Encoding을 적용합니다. 이 인코딩은 입력의 각 스칼라 성분 $p$에 대해, 다음과 같이 정의된 함수를 적용합니다:

$$
\gamma(p) = \left[
\sin(2^0 \pi p), \cos(2^0 \pi p), \ldots,
\sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p)
\right]
$$

이 과정은 입력 스칼라 한 개를 $2L$ 차원의 벡터로 확장하며, 3차원 벡터에 적용될 경우 총 $6L$ 차원으로 증가합니다. NeRF에서는 위치 정보에 대해 $L=10$을 사용하여 총 60차원으로, 시점 방향에 대해서는 $L=4$를 사용하여 24차원으로 확장합니다. 이로 인해 네트워크는 인접한 두 좌표 간의 미세한 차이도 고차원 공간에서 뚜렷하게 구분할 수 있게 되며, 복잡한 장면 구조나 텍스처의 고주파 정보를 효과적으로 학습할 수 있게 됩니다.
특히 시점 방향은 $(\theta,\phi)$와 같은 극좌표 형태보다는, 3D 공간에서의 실제 방향성을 직관적으로 나타내기 위해 단위 벡터 $(d_x,d_y,d_z)$ 형태로 변환되어 사용됩니다. 이 방향 벡터 역시 위에서 설명한 positional encoding 과정을 거쳐 24차원으로 확장된 후, 후반부 MLP에 사용됩니다.

<Image src="/images/paper-review/NeRF/architecture.jpeg" width={800} height={450} alt="..." />

이제 NeRF의 전체 MLP 구조는 두 단계로 나뉩니다. 첫 번째 단계는 위치 인코딩 벡터 
$\gamma(x) \in \mathbb(R^{60})$를 입력받아 밀도와 중간 특성 벡터를 출력하는 geometry branch입니다. 이 브랜치는 256 채널의 Fully Connected 레이어 4개를 연속으로 통과한 뒤, 네 번째 레이어 출력과 원래 입력 
$\gamma(x)$를 skip connection으로 다시 결합합니다. 결합된 316차원 벡터는 추가로 4개의 FC 레이어를 더 거쳐, 최종적으로 밀도 값과 중간 표현을 출력합니다. 이 중 밀도는 ReLU 함수를 통해 양수로 제한되며, 이는 물체 내부와 공기를 구분하기 위한 물리적 제약을 반영한 것입니다.

두 번째 단계는 중간 표현 벡터 $f$와 viewing direction의 positional encoding 
$\gamma(d) \in \mathbb{R^{24}}$를 결합하여 색상을 예측하는 color branch입니다. 결합된 280차원 벡터는 256 → 128 유닛의 FC 레이어 두 개를 지나며, 마지막으로 sigmoid 함수를 거쳐 
RGB를 출력합니다. 이처럼 시점 정보는 late-fusion 방식으로 적용되어, Lambertian 가정이 아닌 시점 의존적 반사량이나 광택 표현 등 비-Lambertian 효과도 포괄적으로 모델링할 수 있습니다.

<Image src="/images/paper-review/NeRF/lambertian_effect.png" width={800} height={450} alt="..." />

###
---

### 6. Hierarchical Sampling (Coarse → Fine)

NeRF는 각 픽셀에 해당하는 ray를 따라 연속적인 지점에서 밀도와 색상을 예측합니다. 하지만 모든 위치를 균등하게 샘플링하면 경계나 물체 내부 구조를 포착하지 못하고, 연산도 비효율적으로 낭비됩니다. 이를 해결하기 위해 NeRF는 중요한 구간에 더 많은 샘플을 집중시키는 Hierarchical Sampling 전략을 도입합니다.

먼저 coarse 단계에서는 ray 위의 일정 간격으로 $N_c = 64$개의 샘플을 생성하고, 각 구간 내에서 무작위로 하나의 지점을 선택하는 Stratified Sampling을 사용합니다. 이 방식은 구간 간의 균등성과 구간 내의 무작위성을 동시에 확보하여 aliasing과 overfitting을 줄입니다. 각 샘플 위치 $z_i$에서 예측된 밀도 $\sigma_i$를 통해 해당 구간의 중요도를 추정하고, 이로부터 weight를 계산합니다.


#### 2차 샘플링의 핵심: Importance Sampling

이제 coarse에서 얻은 weight를 정규화하여 확률 밀도 함수(PDF)를 구성하고, 이를 누적하여 누적 분포 함수(CDF)를 만듭니다. 이후 이 CDF를 이용해 중요한 구간에서 더 많은 샘플을 생성하는 계층적 샘플링을 수행합니다.
이는 단순히 uniform 분포에서 난수를 생성한 후, 이를 CDF에 대응시키기만 해도 확률이 높은 영역에 샘플이 자동으로 몰리게 되는 매우 효율적인 샘플링 방식입니다.

<Image src="/images/paper-review/NeRF/importance.png" alt="importance" width={800} height={450} />

<HighlightText lightColor="red.300" darkColor="#ffb163" fontSize="xl">
여기서 중요한 질문이 하나 생깁니다.  
단순히 [0, 1] 구간에서 uniform하게 샘플 $u \sim \mathcal{U}(0, 1)$ 을 뽑을 뿐인데, 어떻게 밀도 기반의 샘플링이 가능한가?
</HighlightText>


그 해답이 바로 Inverse Transform Sampling입니다.  
CDF가 단조 증가 함수라는 성질을 활용하면, uniform하게 뽑은 $u$를 CDF의 역함수로 변환함으로써, 확률이 높은 구간일수록 더 많은 샘플을 생성할 수 있습니다.

이 방식의 핵심 원리는 다음과 같습니다:

- PDF의 특정 구간에서 값이 높다는 것은, 그 구간이 중요한 영역(예: 물체가 존재하는 영역)이라는 뜻입니다.
- 이 PDF를 누적하여 얻은 CDF는 해당 영역의 확률 질량을 반영한 형태가 됩니다.
- CDF에서 $u = 0.1, 0.2, \dots$ 등 일정 간격의 값을 역변환(invert)하면, 기울기가 가파른 구간(=PDF 값이 큰 구간)이 더 많은 hit를 유도하게 되어, 그 구간에서 샘플이 더 촘촘하게 생성됩니다.


#### Fine Sampling: Inverse Transform Sampling 구현

다음은 이 원리를 실제 코드에 적용한 핵심 부분입니다. CDF를 기반으로 균등 분포 샘플 $u$를 적절한 구간으로 매핑하여, 그에 대응하는 샘플 $z$를 선형 보간 방식으로 계산합니다:

$$
z = z_{\text{below}} + \frac{u - \text{CDF}_{\text{below}}}{\text{CDF}_{\text{above}} - \text{CDF}_{\text{below}}} \cdot (z_{\text{above}} - z_{\text{below}})
$$

NeRF 구현에서는 다음과 같은 `sample_pdf` 함수로 이 과정을 수행합니다:

```python
def sample_pdf(bins, weights, N_samples, det=False):
    # Normalize weights → PDF, then integrate → CDF
    weights += 1e-5  # prevent division by zero
    pdf = weights / tf.reduce_sum(weights, axis=-1, keepdims=True)
    cdf = tf.concat([tf.zeros_like(pdf[..., :1]), tf.cumsum(pdf, axis=-1)], axis=-1)

    # Uniform samples in [0, 1] → to be mapped via inverse CDF
    if det:
        u = tf.linspace(0., 1., N_samples)
        u = tf.broadcast_to(u, tf.shape(cdf)[..., :-1] + [N_samples])
    else:
        u = tf.random.uniform(tf.shape(cdf)[..., :-1] + [N_samples])

    # Locate CDF bins for each u
    inds = tf.searchsorted(cdf, u, side='right')
    below = tf.maximum(0, inds - 1)
    above = tf.minimum(cdf.shape[-1] - 1, inds)
    inds_g = tf.stack([below, above], axis=-1)

    # Linear interpolation between CDF bounds
    cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(cdf.shape) - 2)
    bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(bins.shape) - 2)
    denom = tf.where(cdf_g[..., 1] - cdf_g[..., 0] < 1e-5, 1., cdf_g[..., 1] - cdf_g[..., 0])
    t = (u - cdf_g[..., 0]) / denom
    return bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])

```


---

### 7. 손실 함수 / Loss Function

NeRF는 3차원 장면의 복원 결과가 실제 이미지와 얼마나 일치하는지를 기준으로 모델 파라미터(MLP 가중치)를 최적화합니다. 이를 위해 NeRF는 ray를 따라 렌더링한 예측 색상 $\hat{C}(r)$과 실제 이미지의 정답 색상 $C^*(r)$ 간의 차이를 최소화하는 방식으로 학습이 이루어집니다. 이때 사용되는 손실 함수는 가장 일반적인 회귀 목적의 손실인 $L_2 손실$이며 각 픽셀에 대해 예측과 정답의 제곱 오차를 누적하여 전체 네트워크를 학습합니다.

NeRF는 학습 시 두 개의 MLP 네트워크 — coarse와 fine — 를 사용하며, 이들 모두에서 손실을 각각 계산하여 최종 손실로 더해줍니다. 이렇게 하는 이유는 coarse 네트워크가 fine 네트워크의 샘플링 분포에 영향을 주기 때문입니다. coarse는 단순히 중간 결과를 얻는 데 그치지 않고, 정보 밀도가 높은 영역을 파악하여 fine 네트워크가 더 나은 위치에서 샘플링할 수 있도록 도와주는 역할을 합니다. 따라서 coarse 네트워크 역시 별도의 손실을 통해 정확히 학습될 필요가 있습니다.

NeRF에서 사용하는 전체 손실 함수는 다음과 같습니다:

$$
\mathcal{L} = \sum_{\mathbf{r}} \left( \| C_c(\mathbf{r}) - C^*(\mathbf{r}) \|_2^2 + \| C_f(\mathbf{r}) - C^*(\mathbf{r}) \|_2^2 \right)
$$

$C_c(\mathbf{r})$: coarse 네트워크로부터 렌더링된 색상

$C_f(\mathbf{r})$: fine 네트워크로부터 렌더링된 색상

$C^*(\mathbf{r})$: ground truth 색상

이 정보를 기반으로 학습 루프는 다음과 같은 방식으로 진행됩니다:

- 여러 이미지 중 일부를 무작위로 선택한 뒤, 각 이미지에서 수천 개의 픽셀을 랜덤하게 샘플링합니다.

- 각 픽셀은 카메라 내부 파라미터(intrinsics)와 포즈를 통해 ray로 변환되며, 이 ray는 3D 공간을 통과하면서 여러 위치에서 색상 및 밀도를 예측하게 됩니다.

- coarse 네트워크는 균일 샘플링된 위치에 대해 밀도와 색상을 예측하고, 이를 바탕으로 fine 네트워크가 추가 샘플을 생성하여 보다 정밀한 예측을 수행합니다.

- 두 네트워크로부터 얻어진 예측 색상은 ground truth 픽셀 색상과 비교되며, 그 차이로부터 gradient가 계산되어 MLP의 가중치가 업데이트됩니다.

이러한 학습 과정을 수천, 수만 번 반복하면서, NeRF는 점차적으로 3D 장면의 구조와 색상 정보를 정밀하게 복원할 수 있는 모델로 학습됩니다. coarse–fine 구조, 볼륨 렌더링 기반의 미분 가능한 프레임워크, 위치 기반 쿼리 모델이라는 특징 덕분에 NeRF는 제한된 이미지 수만으로도 정교한 3차원 재구성을 가능하게 합니다.


---

### 8. 실험 및 결과 / Experiments and Results

### Implementation Details
- 배치 크기: 4096개의 ray 샘플 포인트 사용
- Optimizer: Adam
- $𝛽_1=0.9, 𝛽_2=0.999$
- learning rate = $5×10^{-4}$

<Image src="/images/paper-review/NeRF/comparisons_0.png" width={800} height={450} alt="..." />
**NeRF Results**

<Image src="/images/paper-review/NeRF/comparisons_1.png" width={800} height={450} alt="..." />
**NeRF Results**

<Image src="/images/paper-review/NeRF/comparisons_2.png" width={800} height={450} alt="..." />
**NeRF Results**


NeRF 논문에서는 생성된 이미지의 품질을 평가하기 위해 PSNR, SSIM, LPIPS 세 가지 지표를 사용합니다.

- PSNR(Peak Signal-to-Noise Ratio)은  
 원본 이미지와 복원 이미지 간의 평균 제곱 오차(MSE)를 기반으로 계산되며, 수치가 높을수록 복원 정확도가 높고 왜곡이 적다는 것을 의미합니다. 단위는 데시벨(dB)이며, 주로 화질 손실 정도를 정량적으로 평가할 때 사용됩니다.

- SSIM(Structural Similarity Index Measure)은  
두 이미지 간의 밝기, 대비, 구조 정보를 종합적으로 비교하여 시각적 유사도를 측정하는 지표입니다. 값의 범위는 0에서 1 사이이며, 1에 가까울수록 사람의 눈에 보기에도 유사한 이미지로 평가됩니다.

- LPIPS(Learned Perceptual Image Patch Similarity)는  
사전 학습된 딥러닝 네트워크를 통해 이미지 간의 인지적 차이를 측정하는 지표입니다. 값이 낮을수록 두 이미지가 시각적으로 더 유사하다는 뜻이며, 세밀한 질감이나 시각적 품질 차이를 평가하는 데 효과적입니다.

<Image src="/images/paper-review/NeRF/ablation_study.png" width={800} height={450} alt="..." />

**Ablation Study**

## 9. 결론 / Conclusion

NeRF는 정적 3D 장면을 연속적인 위치, 시점 방향로 표현하여, 기존의 복셀, 메쉬, 포인트 클라우드 기반 방식보다 훨씬 더 높은 품질의 novel view synthesis를 가능하게 했습니다. NeRF는 특히 sparse한 이미지 입력만으로도 고해상도 이미지를 생성해내는 혁신적인 접근으로, 신경망 기반 3D 재구성의 패러다임을 크게 전환시켰습니다.
하지만 NeRF는 실용적인 응용을 위해 해결해야 할 여러 한계를 지니고 있습니다:

- 느린 학습 및 렌더링 속도
NeRF는 수십만 번의 iteration을 거쳐야 장면 하나를 학습할 수 있어, 시간과 연산 자원이 과도하게 요구됩니다.
→ Plenoxels, Instant-NGP 등은 연산 효율을 극적으로 개선하는 방향으로 발전했습니다.

- 정적 장면에 국한된 성능
동적 객체나 장면에서는 재구성 정확도가 떨어지며 노이즈가 많아집니다.
→ D-NeRF, Nerfies, HyperNeRF는 시간 축을 포함한 장면 모델링을 시도합니다.

- 조명 변화에 취약
NeRF는 동일 물체라도 촬영 환경(조도, 날씨 등)이 다르면 일반화가 어렵습니다.
→ NeRF in the Wild, NeRV, NeRD 등은 다양한 조건을 반영하는 모델로 확장됩니다.

- 범용성 부족 (Generalization)
하나의 NeRF 모델은 하나의 장면에 특화되어, 다른 장면에 직접 활용할 수 없습니다.
→ pixel-NeRF, GIRAFFE 등은 few-shot 또는 zero-shot generalization을 시도합니다.

- 다량의 입력 이미지 요구
수십~수백 장의 시점 이미지가 필요하다는 점은 실사용에서 비효율적입니다.
→ DietNeRF, RegNeRF 등은 소수의 이미지로도 재구성 가능한 모델을 개발 중입니다.

- 카메라 파라미터 의존성
학습을 위해 정확한 camera pose 정보가 반드시 필요하다는 점은 일반 사용자에겐 큰 장벽입니다.
→ iNeRF, BARF, SCNeRF 등은 포즈 추정 또는 공동 학습 방식을 통해 이 제약을 줄이려 합니다.

#### 앞으로의 방향성
NeRF는 단일 논문 이상의 가치를 가지며, 이후 수많은 파생 연구들을 촉진시킨 3D 장면 표현의 새로운 시대를 연 전환점이라 할 수 있습니다. 현재 NeRF 기반 기술은 SLAM, object reconstruction, scene editing, generative modeling 등 다양한 3D-aware task에 핵심 구성 요소로 적용되고 있으며, 앞으로도 실시간성, 범용성, 확장성 측면에서 지속적인 발전이 예상됩니다.
NeRF는 "신경망 기반 장면 재구성"이라는 개념을 명확하게 정립하고 실제로 구현해낸 첫 사례 중 하나로, 앞으로의 컴퓨터 비전 및 컴퓨터 그래픽스 연구에서 반드시 이해하고 넘어가야 할 핵심 기술적 자산이라 생각됩니다.