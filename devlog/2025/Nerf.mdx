---
title: "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
date: "2025-05-04"
summary: "paper review"
---

> 2020년 발표된 NeRF 논문  
> _[Mildenhall et al., ECCV 2020](https://arxiv.org/abs/2003.08934)_ 을 상세히 분석한 리뷰입니다.  
> [[paper]](https://arxiv.org/abs/2003.08934), [[project]](https://www.matthewtancik.com/nerf), [[GitHub]](https://github.com/bmild/nerf)  
> Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng  
> UC Berkeley | Google Research | UC San Diego   
> 2020년 3월 19일


## 목차 / Table of Contents

1. [서론 Introduction](#1)
2. [NRF 표현 Neural Representation](#2)
3. [볼륨 렌더링 Volume Rendering](#3)
4. [NeRF 최적화 Optimization](#4)
5. [실험 및 결과 Results](#5)
6. [어블레이션 연구 Ablation Studies](#6)
7. [한계 및 토의 Discussion](#7)
8. [개인 인사이트 Insights](#8)
9. [결론 Conclusion](#9)

---
<Image
  src="/images/paper-review/NeRF/overview-nerf.png"
  alt="NeRF Overview"
  width={800}
  height={450}
/>


&nbsp;&nbsp;최근 진행한 로봇 프로젝트를 통해, 로봇이 물체를 집거나 조작하는 과정에서 정확한 3차원 장면 복원 없이는 정밀한 제어가 어렵다는 점을 실감했습니다. 단순히 대상의 위치나 윤곽만 아는 것에 그치지 않고, 형상, 깊이, 반사, 투명도 등 복합적인 시각 정보를 정밀하게 복원할 수 있어야 실제 환경에서 안정적인 인지가 가능하다는 사실을 경험했습니다. 이러한 문제의식 속에서, 복잡한 장면을 연속적인 고차원 표현으로 정밀하게 모델링하는 기술들에 관심을 갖게 되었고 Neural Radiance Fields (NeRF)라는 접근 방식을 접하게 되었습니다. 처음에는 하나의 기법으로 등장했지만, 지금은 NeRF 자체가 3D 신경 표현 분야 전체의 출발점이자 기준점으로 자리잡았다고 생각합니다.  
&nbsp;&nbsp;이번 글에서는 NeRF를 처음 제안한 논문을 중심으로 그 핵심 아이디어와 수식 구조를 설명하고, 이어서 이 아이디어가 어떻게 발전되어 왔는지 주요 후속 연구들을 함께 정리해보려 합니다. 즉, 기초 개념 정리부터 기술 흐름까지 정리해보고자 합니다.

---

## 1. Introduction

&nbsp;&nbsp;NeRF(Neural Radiance Fields)가 처음 제안된 목적은 Novel View Synthesis, 즉 주어진 시점에서 촬영된 이미지들만을 가지고 보지 못한 새로운 시점의 이미지를 rendering하는 것입니다. 이 문제는 컴퓨터 비전과 그래픽스 분야에서 오랫동안 연구되어온 과제로, sparse한 이미지 관측만으로도 연속적인 시점 변화에 대응하는 realistic한 결과를 생성하는 것이 핵심입니다.  
기존에는 view interpolation이나 mesh, voxel, point cloud 기반 방식들이 사용되었지만, 이들은 <span style={{ color: '#ffb163' }}>**막대한 메모리와 계산 비용, 시점 변화에 따른 표현력 부족이라는**</span> 근본적인 한계를 지니고 있었습니다.  
&nbsp;&nbsp;NeRF는 이러한 문제를 해결하기 위해 장면을 연속적인 5차원 함수로 모델링합니다. 3D 공간상의 위치 $(x, y, z)$와 시점 방향 $(\theta, \phi)$를 입력으로 받아, 해당 지점의 밀도 $\sigma$와 색상 $c(x,d)$를 출력하는 MLP를 학습합니다. 이 표현은 단순한 RGB 이미지와 카메라 포즈 정보만을 입력으로 받아, 각 픽셀을 관통하는 ray를 따라 샘플링된 point들의 color와 density를 예측하고, 이를 미분 가능한 volume rendering으로 누적하여 실사에 가까운 novel view를 합성합니다.  
 이러한 접근 방식은 discrete한 입력(view image들)만 가지고도 연속적인 시점(view direction)에 대응하는 결과를 만들어낼 수 있다는 점에서, <span style={{ color: '#ffb163' }}>**continuous scene representation**</span>이라는 새로운 패러다임을 열었고, 이후 다양한 후속 연구의 출발점이 되었습니다.

---

###  NeRF의 핵심 구성 요소

#### **<span style={{ color: '#ffb163' }}>1. 연속적인 5D 표현</span>** 
&nbsp;&nbsp;NeRF는 3D 위치 $(x,y,z)$와 시점 방향 $(θ,ϕ)$로 이루어진 5D 좌표를 입력으로 받아,위치에 따른 volume density $\sigma$, view-dependent radiance $c(x,d)$를 출력하는 MLP를 학습합니다.

#### **<span style={{ color: '#ffb163' }}>2. 미분 가능한 볼륨 렌더링</span>**
&nbsp;&nbsp;볼륨 렌더링은 광선에 따라 샘플링된 위치의 색상 및 밀도를 누적하여 최종 픽셀 색상을 계산합니다. 이 과정은 미분 가능하여, 렌더링된 이미지와 실제 이미지 간의 오차를 최소화하는 방식으로 MLP를 학습할 수 있습니다. 또한, 연산 효율성과 정밀도를 동시에 확보하기 위해, coarse → fine 구조의 계층적 샘플링(Hierarchical Sampling) 기법을 적용합니다.


#### **<span style={{ color: '#ffb163' }}>3. 고주파 학습을 위한 Positional Encoding</span>**
&nbsp;&nbsp;MLP는 고주파 신호(세밀한 경계, 미세 구조 등)를 학습하기 어렵다는 문제가 있습니다. 이를 극복하기 위해 NeRF는 입력 5D 좌표에 사인·코사인 기반 Positional Encoding을 적용하여 고차원으로 임베딩합니다.

---

<span style={{ color: '#f1fa8c', fontWeight: 'bold', fontSize: '1.4rem' }}> 핵심 정리 (요약)</span>

- <span style={{ color: '#8be9fd' }}>5D MLP 기반 연속 장면 표현</span>
- <span style={{ color: '#8be9fd' }}>미분 가능한 볼륨 렌더링으로 이미지와 포즈만으로 학습 가능</span>
- <span style={{ color: '#8be9fd' }}>Positional Encoding을 통해 고주파 세부 표현 학습 가능</span>
- <span style={{ color: '#8be9fd' }}>Hierarchical Samplin으로 효율적인 학습 구조 구현</span>


---

## 2. NeRF의 동작 메커니즘: 전체 파이프라인 분석

<Image src="/images/paper-review/NeRF/pipeline.jpg" alt="NeRF pipeline" width={800} height={450} />
>  &nbsp;&nbsp;NeRF의 입력부터 출력까지의 전체 파이프라인 과정을 시각적으로 요약한 것이다.

### (a) 5D Input (Position + Direction)
&nbsp;&nbsp;카메라 픽셀을 기준으로, rays를 따라 여러 위치 $(x,y,z)$와 를 샘플링하고, 시점 방향 $(\theta,\phi)$와 결합해 <span style={{ color: '#ffb163' }}>**5D 좌표**</span>로 구성합니다. 이는 MLP $F_\Theta$의 입력이 됩니다. 예를 들어, NeRF 논문에서 사용된 Blender Synthetic Dataset(예: Lego, Drums 등)은 기본 해상도 800×800으로 구성되어 있지만, 실험에서는 --half_res 설정을 통해 400×400 해상도로 줄여 사용하며, 이 경우 각 이미지에서 160,000개의 ray가 생성됩니다. 학습 시에는 이 중 약 <span style={{ color: '#ffb163' }}>4096개의</span> ray를 무작위로 샘플링합니다.  
&nbsp;&nbsp;각 ray에 대해 먼저 64개의 위치를 stratified sampling으로 선택하여 coarse 네트워크에 전달하고, 여기서 얻은 밀도 값을 기반으로 확률 밀도 함수를 구성하여 중요도 기반으로 128개의 위치를 추가 샘플링하여 fine 네트워크에 입력합니다. 따라서 각 ray는 최대 192개의 위치에서 예측이 수행됩니다.  
&nbsp;&nbsp;이때 ray의 origin과 direction은 <span style={{ color: '#ffb163' }}>**COLMAP과**</span> 같은 SfM 도구를 통해 복원한 카메라의 extrinsic matrix (camera-to-world) 및 intrinsic parameters(focal length, principal point 등)를 통해 계산되며, 일반적으로 이 pose 정보는 transforms.json 파일로 저장됩니다.

### (b) Output: Color + Density
&nbsp;&nbsp;NeRF의 MLP는 입력된 위치 및 방향 정보를 바탕으로, 해당 지점의 부피 밀도(volume density)와 방향 의존적 색상(view-dependent RGB color)을 출력합니다. 이는 장면의 광학적 특성—예를 들어, 불투명한 물체의 외곽선이나 반사 특성 등—을 정밀하게 표현하는 데 핵심적인 역할을 합니다.  
&nbsp;&nbsp;밀도 $\sigma$는 위치 $(x,y,z)$에만 의존하며, 광선이 해당 위치를 통과할 때 얼마나 많은 색상 정보가 누적되는지를 나타냅니다. 이 값은 **장면의 기하 구조(geometry)**를 나타내는 요소입니다.  
&nbsp;&nbsp;&nbsp;&nbsp;색상 $\mathbf{c}=(r,g,b)$는 위치뿐 아니라 시점 방향 $(\theta, \phi)$에도 의존합니다. 이로 인해 NeRF는 같은 위치라도 시점에 따라 색상이 달라지는 비-Lambertian 재질까지 표현할 수 있습니다.  
NeRF는 이 두 출력을 하나의 다층 퍼셉트론(MLP)을 통해 예측합니다. 먼저, 3D 위치 정보는 Positional Encoding을 통해 고차원 임베딩된 후 MLP에 입력되고, 이를 바탕으로 밀도 $\sigma$와 중간 feature를 출력합니다. 이후 이 중간 feature와 시점 방향 정보를 결합하여 최종 RGB 색상을 출력합니다.

### (c) Volume Rendering
  NeRF는 단일 지점의 색상과 밀도를 예측하는 것에 그치지 않고, 카메라에서 쏜 하나의 ray가 장면을 통과하며 얻는 누적 색상을 계산해야 합니다. 이를 위해 고전적인 볼륨 렌더링 기법을 사용합니다.

  하나의 ray는 장면을 따라 여러 지점을 샘플링하며, 각 지점에서 예측된 밀도와 색상이 최종 이미지의 한 픽셀에 기여하게 됩니다. 이때 각 샘플의 기여도는 해당 위치의 투명도와 색상 정보, 그리고 투과율에 따라 결정됩니다.

  이러한 누적 계산은 연속적인 적분을 이산 샘플로 근사하여 처리하며, 전체 과정은 미분 가능하기 때문에 신경망 학습에 사용할 수 있습니다.

### (d) Rendering Loss
렌더링된 색상 $\hat{C}(r)$과 ground truth 간의 $L_2$ loss를 최소화하며 MLP 파라미터 $\Theta$를 학습합니다.
$$
\mathcal{L} = \sum_r \| \hat{C}(r) - C^*(r) \|^2
$$
즉, 카메라 포즈와 함께 주어진 실제 이미지에서 각 픽셀에 대응되는 ray를 따라 볼륨 렌더링을 수행한 후, **예측된 픽셀 색상 $\hat{C}(r)$**과 실제 색상 $C^*(r)$ 간의 차이를 계산하고, 이 오차가 작아지도록 MLP를 학습하는 방식입니다.

  또한 NeRF는 단일 네트워크만 사용하는 것이 아니라, coarse network와 fine network 두 단계의 MLP를 사용하여, 먼저 전체 영역을 대략적으로 예측한 뒤 중요한 구간을 더 정밀하게 샘플링합니다. 이로써 학습 효율성과 표현 정밀도를 동시에 달성할 수 있습니다.

---

## 3. NeRF 구현 세부 절차

####  COLMAP을 통한 카메라 파라미터 복원

<Image src="/images/paper-review/NeRF/image_projection.png" width={800} height={450} alt="..." />

NeRF 구현의 첫 단계는 각 이미지에 대한 camera pose와 intrinsic parameter를 복원하는 일입니다. 이를 위해 Structure-from-Motion(SfM) 기반 오픈소스 도구인 COLMAP을 사용하며, 이는 이미지 간 feature를 자동으로 매칭하고 bundle adjustment를 통해 카메라의 회전 행렬 $R$, 이동 벡터 $t$, 내부 파라미터, 그리고 sparse 3D point cloud까지 추정합니다.  
COLMAP이 제공하는 포즈는 world-to-camera 변환 행렬로 주어지며, 이는 다음과 같은 변환식을 따릅니다:
$$
\mathbf{x}_{\text{cam}} = R \cdot \mathbf{x}_{\text{world}} + t
$$
하지만 NeRF는 각 픽셀에서 ray를 쏘기 위해, camera-to-world (C2W) 형식의 변환이 필요합니다. 이를 위해:
회전 행렬 $R$을 전치시켜 방향을 반전하고,

카메라의 world 위치(origin)를 아래와 같이 계산합니다:
$$
\mathbf{o} = - R^{\top} \cdot t
$$
최종적으로 회전 및 이동을 조합한 4×4 C2W 행렬을 구성하여 **transforms_train.json**에 저장합니다.

각 픽셀에서의 viewing direction(광선 방향)은 COLMAP이 추정한 카메라 내부 파라미터를 이용해 계산됩니다. 픽셀 좌표 $(u, v)$를 카메라 정규화 좌표계로 변환하면:
$$
\left( \frac{u - c_x}{f_x}, \frac{v - c_y}{f_y}, 1 \right)
$$
이렇게 얻어진 방향 벡터는 카메라 기준의 viewing direction이며, 이를 camera-to-world 회전 행렬에 곱해주면 world 좌표계 기준의 ray 방향으로 변환된다. 이후 이 벡터는 정규화(normalization) 과정을 거쳐 단위 벡터로 만들어지고, NeRF의 MLP 입력으로 positional encoding을 적용해 사용된다.

실제 구현에서는 이미지마다 수천 개의 픽셀을 무작위로 선택하여, 각 픽셀에서 시작되는 ray를 생성하였다. 이때 각 ray마다 origin은 카메라 위치에서, direction은 위에서 계산한 벡터를 통해 설정된다. 방향 벡터는 항상 단위 벡터로 정규화되며, MLP에서는 이 벡터의 각 차원에 대해 사인(sin), 코사인(cos) 기반의 positional encoding을 적용함으로써 고주파 정보까지 학습할 수 있도록 한다.

COLMAP은 또한 sparse 3D point cloud를 함께 생성하는데, 이는 NeRF의 학습에는 직접 사용되지 않지만 매우 유용하다. 예를 들어, 이 포인트 클라우드와 카메라 위치를 함께 시각화하면 COLMAP이 잘 복원되었는지를 직관적으로 검증할 수 있다. 실제로는 Open3D 등의 시각화 도구를 사용해 point cloud와 camera poses를 함께 확인함으로써, scene bounds 설정에 참고하거나 비정상적인 포즈 정렬 문제를 사전에 발견할 수 있었다.

최종적으로, 이 모든 정보를 포함한 결과는 transforms_train.json 파일로 저장되며, NeRF 학습 시 이미지 파일 경로 및 해당 이미지의 camera-to-world 변환 행렬이 참조된다. 이 파일의 구조는 다음과 같은 형식을 따른다.


---

####  Ray 정의 및 Stratified Sampling
NeRF에서의 핵심은 2차원 이미지의 각 픽셀로부터 3차원 공간상의 연속적인 지점을 따라 샘플링하고 각 지점들에서 색상(RGB)과 밀도(σ)를 예측하여 다시 2차원 이미지로 투영하는 것입니다. 이 과정을 위해 NeRF는 먼저 각 픽셀마다 하나의 광선(ray) 을 정의합니다. 이 ray는 장면을 관통하며 여러 지점에서 MLP를 통해 장면 정보를 예측하는 기준선 역할을 합니다.
<div style={{ textAlign: 'center' }}>
  <Image
    src="/images/paper-review/NeRF/ray.png"
    alt="NeRF pipeline"
    width={400}
    height={450}
    style={{ display: 'block', margin: '0 auto' }}
  />
</div>

각 ray는 다음과 같은 수식으로 정의됩니다:
$$
r(t) = \mathbf{o} + t ⋅ \mathbf{d}, \quad t \in [t_n, t_f]
$$

- $\mathbf{o}$: 카메라 위치 (COLMAP에서 추출한 camera-to-world matrix에서 translation 성분)
- $\mathbf{d}$: 픽셀을 기준으로 생성된 방향 벡터 (카메라 intrinsic matrix를 통해 계산된 후, world 좌표계로 변환됨)
- $t$: 광선 상에서의 샘플 위치를 지정하는 스칼라 값

이때 t는 ray 상의 샘플링 지점을 의미하며, NeRF는 여러 $t_i$ 값을 따라 $r(t_i)$ 위치에서 MLP를 통해 색상과 밀도를 예측합니다. 하지만 모든 이미지의 모든 픽셀에 대해 ray를 생성하고 계산한다면 학습 시간과 메모리 사용량이 과도하게 커지므로, NeRF는 매 iteration마다 일부 이미지와 일부 픽셀만 무작위로 선택하여 ray를 생성하고 학습에 사용합니다. 이를 통해 효율적인 학습이 가능합니다.

<div style={{ textAlign: 'center' }}>
  <Image
    src="/images/paper-review/NeRF/stratified_sampling.png"
    alt="NeRF pipeline"
    width={400}
    height={450}
    style={{ display: 'block', margin: '0 auto' }}
  />
</div>

> 각 ray는 near–far 구간 $[t_n, t_f]$에서 샘플링되며, 이 범위 내에서 여러 지점에서의 색상과 밀도를 예측합니다


#### Stratified Sampling:

Ray가 정의되면 그 위를 따라 여러 지점을 샘플링해야 합니다. 이때 단순히 $[t_n, t_f]$ 구간에서 균등 간격으로 샘플링하면 특정 패턴이 반복되어 overfitting 또는 aliasing이 발생할 수 있습니다. 이를 방지하고 샘플 분포의 다양성을 확보하기 위해 NeRF는 Stratified Sampling, 즉 계층적 무작위 샘플링 기법을 사용합니다.

이 기법은 다음과 같은 방식으로 작동합니다:
- 전체 샘플링 구간 $[t_n,t_f]$을 $N$개의 동일한 폭을 가진 구간으로 나눈다.
- 각 구간마다, 그 내부에서 하나의 위치 $t_i$를 무작위로 선택한다.

수식으로 표현하면 다음과 같다:
$$
t_i \sim \mathcal{U} \left[ t_n + \frac{i - 1}{N}(t_f - t_n),\; t_n + \frac{i}{N}(t_f - t_n) \right]
$$

이 방식은 전체 구간을 고르게 커버하면서도 각 구간 안에서 무작위성을 도입해 **편향(bias)**과 **분산(variance)**을 동시에 제어할 수 있도록 해줍니다. 이로 인해 NeRF는 더 안정적이고 일반화 가능한 학습을 수행할 수 있습니다.

Stratified Sampling은 수치적 통합(numerical integration) 기법 중 하나인 Monte Carlo Integration의 variance 감소 전략과 동일한 수학적 효과를 가지며, NeRF의 학습 수렴 속도와 품질에 크게 기여합니다. 특히 정적인 카메라 시점에서도 overfitting 없이 전체 장면을 잘 일반화할 수 있도록 돕는 중요한 역할을 합니다.

---

### 3. MLP 입력 및 구조

NeRF는 3차원 장면 내 임의의 지점에서 ray를 따라 관찰되는 색상과 밀도 값을 예측하기 위해 다층 퍼셉트론(MLP)을 사용합니다. 그러나 단순한 위치 벡터 $(x,y,z)$와 시점 방향 벡터 $(\theta,\phi)$만을 저차원 형태로 입력하면, MLP는 경계, 텍스처, 반사와 같은 고주파 정보를 제대로 표현하지 못하는 한계를 가집니다. 이는 Rahaman et al.(2019)의 연구 [On the Spectral Bias of Neural Networks](https://arxiv.org/abs/1806.08734)에서도 지적되었듯, 일반적인 신경망이 낮은 주파수부터 우선 학습하려는 경향(spectral bias)을 가지기 때문입니다. NeRF는 이러한 한계를 극복하기 위해 Positional Encoding을 진행합니다. 본 섹션에서는 그 중 전자인 Positional Encoding과 MLP의 구조적 설계에 대해 자세히 설명합니다. NeRF는 각 ray 상의 샘플링 지점에 대해, 위치 벡터 
$x$와 viewing direction 벡터 $d$를 입력으로 받아 밀도와 색상RGB을 예측합니다. 그러나 이 두 벡터는 단순한 3차원 좌표로는 표현력이 부족하기 때문에, NeRF는 이들을 고차원 공간으로 사인·코사인 변환하는 Positional Encoding을 적용합니다. 이 인코딩은 입력의 각 스칼라 성분 $p$에 대해, 다음과 같이 정의된 주기 함수를 적용합니다:

$$
\gamma(p) = \left[
\sin(2^0 \pi p), \cos(2^0 \pi p), \ldots,
\sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p)
\right]
$$

이 과정은 입력 스칼라 한 개를 $2L$ 차원의 벡터로 확장하며, 3차원 벡터에 적용될 경우 총 $6L$ 차원으로 증가합니다. NeRF에서는 위치 정보에 대해 $L=10$을 사용하여 총 60차원으로, 시점 방향에 대해서는 $L=4$를 사용하여 24차원으로 확장합니다. 이로 인해 네트워크는 인접한 두 좌표 간의 미세한 차이도 고차원 공간에서 뚜렷하게 구분할 수 있게 되며, 복잡한 장면 구조나 텍스처의 고주파 정보를 효과적으로 학습할 수 있게 됩니다.
특히 시점 방향은 $(\theta,\phi)$와 같은 극좌표 형태보다는, 3D 공간에서의 실제 방향성을 직관적으로 나타내기 위해 단위 벡터 $(d_x,d_y,d_z)$ 형태로 변환되어 사용됩니다. 이 방향 벡터 역시 위에서 설명한 positional encoding 과정을 거쳐 24차원으로 확장된 후, 후반부 MLP에 사용됩니다.

<Image src="/images/paper-review/NeRF/architecture.jpeg" width={800} height={450} alt="..." />

이제 NeRF의 전체 MLP 구조는 두 단계로 나뉩니다. 첫 번째 단계는 위치 인코딩 벡터 
$\gamma(x) \in \mathbb(R^60)$를 입력받아 밀도와 중간 특성 벡터를 출력하는 geometry branch입니다. 이 브랜치는 256 채널의 Fully Connected 레이어 4개를 연속으로 통과한 뒤, 네 번째 레이어 출력과 원래 입력 
$\gamma(x)$를 skip connection으로 다시 결합합니다. 결합된 316차원 벡터는 추가로 4개의 FC 레이어를 더 거쳐, 최종적으로 밀도 값과 중간 표현을 출력합니다. 이 중 밀도는 ReLU 함수를 통해 양수로 제한되며, 이는 물체 내부와 공기를 구분하기 위한 물리적 제약을 반영한 것입니다.

두 번째 단계는 중간 표현 벡터 $f$와 viewing direction의 positional encoding 
$\gamma(d) \in \mathbb{R^{24}}$를 결합하여 색상을 예측하는 color branch입니다. 결합된 280차원 벡터는 256 → 128 유닛의 FC 레이어 두 개를 지나며, 마지막으로 sigmoid 함수를 거쳐 
RGB를 출력합니다. 이처럼 시점 정보는 late-fusion 방식으로 적용되어, Lambertian 가정이 아닌 시점 의존적 반사량이나 광택 표현 등 비-Lambertian 효과도 포괄적으로 모델링할 수 있습니다.

<Image src="/images/paper-review/NeRF/lambertian_effect.png" width={800} height={450} alt="..." />

---

### 5. 볼륨 렌더링과 색상 예측

NeRF는 고전적인 볼륨 렌더링(volume rendering) 기법을 기반으로 최종 이미지의 픽셀 색상을 계산합니다. 이는 카메라에서 발사된 **ray(광선)**가 3차원 장면을 통과하면서 다양한 지점을 지나고, 각 지점에서의 색상과 밀도가 누적되어 하나의 픽셀로 표현된다는 물리 기반 렌더링 원리에 바탕을 둡니다. 특히 NeRF는 이 전체 과정을 **미분 가능(differentiable)**하게 구성하여, 신경망 학습 과정에 통합할 수 있도록 설계되어 있습니다.
카메라에서 출발한 ray는 $r(t)=o+td$로 정의되며, 여기서 $o$는 ray의 시작점(카메라 위치), $d$는 ray의 방향 벡터입니다. 이 ray가 지나가는 동안 위치 $t$에서의 밀도 
$\sigma(t)$와 색상 $c(t,d)$는 NeRF의 MLP를 통해 예측되며, 이들을 바탕으로 픽셀에 최종적으로 보이는 색상 $C(r)$는 다음과 같은 연속 적분 형태로 표현됩니다:

$$
C(r) = \int_{t_n}^{t_f} T(t) \, \sigma(t) \, c(t, d) \, dt
$$

- $\sigma(t)$: 위치 $t$에서의 밀도
- $\mathbf{c}(t, \mathbf{d})$: 위치 $t$에서 시점 방향 $\mathbf{d}$에 따른 색상

여기서 $T(t)$는 시작점부터 $t$까지의 누적 투과율(transmittance)로, ray가 $t$ 지점까지 도달할 수 있을 확률을 나타냅니다. 이는 다음과 같이 정의됩니다:

$$
T(t) = \exp\left( -\int_{t_n}^t \sigma(s) \, ds \right)
$$
이 수식은 광선이 장면을 통과하면서 얼마나 감쇠되는지를 근사하는 것으로, 밀도가 높을수록 광선이 더 빨리 흡수되어 색상 기여도가 줄어듦을 의미합니다.

그러나 실제 구현에서는 이 연속 적분을 직접 계산할 수 없기 때문에, NeRF는 이산화를 통해 $N$개의 샘플 지점을 사용한 근사합으로 계산합니다. 이때 픽셀 색상은 다음과 같은 형태로 근사됩니다:
$$
C(r) \approx \sum_{i=1}^{N} w_i \, c_i,\quad
w_i = T_i \left( 1 - \exp(-\sigma_i \delta_i) \right)
$$

- $\delta_i$: 연속 샘플 간 간격
- $T_i$: $i$번째 지점까지의 누적 투과율

$w_i$는 해당 위치가 최종 색상에 얼마나 기여하는지를 나타내는 가중치 역할을 합니다. 결국, NeRF는 ray 상의 각 지점의 색상과 밀도를 모두 고려하여 부드럽게 혼합된 색상 결과를 만들어냅니다.


### 6. Hierarchical Sampling (Coarse → Fine)

기본적으로 NeRF는 ray 상의 포인트들을 균일한 간격으로 샘플링하여 색상과 밀도를 계산합니다. 하지만 모든 위치에서 동일한 해상도로 샘플링할 경우, 장면의 복잡한 구조를 효과적으로 복원하기 어렵고, 계산 효율 측면에서도 불필요한 연산이 많아질 수 있습니다. 이를 해결하기 위해 NeRF는 두 단계로 구성된 계층적 샘플링(Hierarchical Sampling) 전략을 도입합니다. 이 구조는 장면의 중요 구간에 더 많은 샘플을 집중시켜 표현력을 높이는 방식입니다.

먼저, 첫 번째 네트워크인 Coarse Network는 균일한 간격으로 ray 상의 $N_c=64$개의 지점을 샘플링하고, 각 지점의 밀도 $\sigma_i$​
 를 예측합니다. 이 예측된 밀도를 바탕으로 해당 ray 상의 어떤 구간이 장면 구성에 더 많은 정보를 담고 있는지를 추정합니다.

이후 두 번째 네트워크인 Fine Network에서는, Coarse Network의 예측 결과로부터 **확률 밀도 함수(PDF)**를 구축하여, 정보량이 높은 위치에 집중적으로 추가 샘플링을 수행합니다. 이 과정에서는 $N_f=128$개의 새로운 샘플 지점을 선택하고, 기존의 64개 샘플과 결합하여 총 $N=192$개의 포인트에서 색상과 밀도를 정밀하게 예측하게 됩니다.
이러한 샘플링 방식은 전체 ray 구간에서의 연산을 균등하게 분산시키기보다, 중요 구간에 연산 자원을 집중함으로써 더 정밀하고 세밀한 묘사가 가능하게 합니다. 즉, 고품질 재구성을 위해 계산 효율과 표현 정밀도의 균형을 맞춘 전략입니다.

- Coarse 단계에서는 균일한 stratified sampling을 사용해 전체 ray를 거칠게 스캔하고,

- Fine 단계에서는 그 결과를 기반으로 정보 밀도가 높은 구간을 더 촘촘히 샘플링합니다.

이 계층적 구조 덕분에 NeRF는 제한된 계산 자원으로도 매우 정교한 3D 장면 복원을 수행할 수 있습니다.

---

### 7. 손실 함수 및 학습 방식

NeRF는 3차원 장면의 복원 결과가 실제 이미지와 얼마나 일치하는지를 기준으로 모델 파라미터(MLP 가중치)를 최적화합니다. 이를 위해 NeRF는 ray를 따라 렌더링한 예측 색상 $\hat{C}(r)$과 실제 이미지의 정답 색상 $C^*(r)$ 간의 차이를 최소화하는 방식으로 학습이 이루어집니다. 이때 사용되는 손실 함수는 가장 일반적인 회귀 목적의 손실인 $L_2 손실$이며 각 픽셀에 대해 예측과 정답의 제곱 오차를 누적하여 전체 네트워크를 학습합니다.

NeRF는 학습 시 두 개의 MLP 네트워크 — coarse와 fine — 를 사용하며, 이들 모두에서 손실을 각각 계산하여 최종 손실로 더해줍니다. 이렇게 하는 이유는 coarse 네트워크가 fine 네트워크의 샘플링 분포에 영향을 주기 때문입니다. coarse는 단순히 중간 결과를 얻는 데 그치지 않고, 정보 밀도가 높은 영역을 파악하여 fine 네트워크가 더 나은 위치에서 샘플링할 수 있도록 도와주는 역할을 합니다. 따라서 coarse 네트워크 역시 별도의 손실을 통해 정확히 학습될 필요가 있습니다.

NeRF에서 사용하는 전체 손실 함수는 다음과 같습니다:

$$
\mathcal{L} = \sum_{\mathbf{r}} \left( \| C_c(\mathbf{r}) - C^*(\mathbf{r}) \|_2^2 + \| C_f(\mathbf{r}) - C^*(\mathbf{r}) \|_2^2 \right)
$$

$C_c(\mathbf{r})$: coarse 네트워크로부터 렌더링된 색상

$C_f(\mathbf{r})$: fine 네트워크로부터 렌더링된 색상

$C^*(\mathbf{r})$: ground truth 색상

이 정보를 기반으로 학습 루프는 다음과 같은 방식으로 진행됩니다:

- 여러 이미지 중 일부를 무작위로 선택한 뒤, 각 이미지에서 수천 개의 픽셀을 랜덤하게 샘플링합니다.

- 각 픽셀은 카메라 내부 파라미터(intrinsics)와 포즈를 통해 ray로 변환되며, 이 ray는 3D 공간을 통과하면서 여러 위치에서 색상 및 밀도를 예측하게 됩니다.

- coarse 네트워크는 균일 샘플링된 위치에 대해 밀도와 색상을 예측하고, 이를 바탕으로 fine 네트워크가 추가 샘플을 생성하여 보다 정밀한 예측을 수행합니다.

- 두 네트워크로부터 얻어진 예측 색상은 ground truth 픽셀 색상과 비교되며, 그 차이로부터 gradient가 계산되어 MLP의 가중치가 업데이트됩니다.

이러한 학습 과정을 수천, 수만 번 반복하면서, NeRF는 점차적으로 3D 장면의 구조와 색상 정보를 정밀하게 복원할 수 있는 모델로 학습됩니다. coarse–fine 구조, 볼륨 렌더링 기반의 미분 가능한 프레임워크, 위치 기반 쿼리 모델이라는 특징 덕분에 NeRF는 제한된 이미지 수만으로도 정교한 3차원 재구성을 가능하게 합니다.


---

## 4. 실험 및 결과

### Datasets

- Diffuse Synthetic 360° (DeepVoxels)

간단한 구조와 Lambertian 반사(모든 방향으로 균일 반사)를 가지는 4개 객체

반구형 궤적을 따라 512×512 해상도로 촬영

기존 연구와의 정량 비교를 위한 기준 데이터셋

- Realistic Synthetic 360° (저자 자체 제작)

현실적인 반사 성질을 가지는 8개 객체 (non-Lambertian surface)

대부분 반구 궤적, 일부는 구형 궤적을 따라 800×800 해상도로 촬영

복잡한 조명과 반사를 포함한 신중하게 구성된 합성 데이터셋

- Real Forward-Facing (LLFF)

실제 장면을 전방위(viewing cone)에서 촬영한 이미지들

카메라의 움직임이 한쪽 방향(전방)으로 제한된 복잡한 실세계 데이터셋

### Implementation Details
- 배치 크기: 4096개의 ray 샘플 포인트 사용
- Optimizer: Adam
- $𝛽_1=0.9, 𝛽_2=0.999$
- learning rate = $5×10^{-4}$

<Image src="/images/paper-review/NeRF/comparisons_0.png" width={800} height={450} alt="..." />
**NeRF Results**

<Image src="/images/paper-review/NeRF/comparisons_1.png" width={800} height={450} alt="..." />
**NeRF Results**

<Image src="/images/paper-review/NeRF/comparisons_2.png" width={800} height={450} alt="..." />
**NeRF Results**


NeRF 논문에서는 생성된 이미지의 품질을 평가하기 위해 PSNR, SSIM, LPIPS 세 가지 지표를 사용합니다.

PSNR(Peak Signal-to-Noise Ratio)은 원본 이미지와 복원 이미지 간의 평균 제곱 오차(MSE)를 기반으로 계산되며, 수치가 높을수록 복원 정확도가 높고 왜곡이 적다는 것을 의미합니다. 단위는 데시벨(dB)이며, 주로 화질 손실 정도를 정량적으로 평가할 때 사용됩니다.

SSIM(Structural Similarity Index Measure)은 두 이미지 간의 밝기, 대비, 구조 정보를 종합적으로 비교하여 시각적 유사도를 측정하는 지표입니다. 값의 범위는 0에서 1 사이이며, 1에 가까울수록 사람의 눈에 보기에도 유사한 이미지로 평가됩니다.

LPIPS(Learned Perceptual Image Patch Similarity)는 사전 학습된 딥러닝 네트워크를 통해 이미지 간의 인지적 차이를 측정하는 지표입니다. 값이 낮을수록 두 이미지가 시각적으로 더 유사하다는 뜻이며, 세밀한 질감이나 시각적 품질 차이를 평가하는 데 효과적입니다.

<Image src="/images/paper-review/NeRF/ablation_study.png" width={800} height={450} alt="..." />

**Ablation Study**

## 8. 결론

NeRF는 복잡한 기하와 재질을 하나의 MLP로 연속적으로 표현하는 breakthrough였으며, 이후 수많은 확장 논문들이 등장하며 딥러닝 기반 3D 장면 표현의 새로운 기준이 되었다.

## 참고자료

- [Original NeRF Paper](https://arxiv.org/abs/2003.08934)
- [Official Code (GitHub)](https://github.com/bmild/nerf)
- [Instant-NGP (2022)](https://github.com/NVlabs/instant-ngp)
- [NeRF-W (2021)](https://arxiv.org/abs/2012.09730)