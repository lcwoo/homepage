---
title: "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
date: "2025-05-04"
summary: "paper review"
---

> 이 문서는 2020년 발표된 NeRF 논문([Mildenhall et al., ECCV 2020](https://arxiv.org/abs/2003.08934))을 상세히 분석한 리뷰입니다.
ECCV 2020. [Paper](https://arxiv.org/abs/2003.08934) [Page](https://www.matthewtancik.com/nerf) [Github](https://github.com/bmild/nerf?tab=readme-ov-file)
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng
UC Berkeley | Google Research | UC San Diego
19 Mar 2020

## 목차 / Table of Contents

1. [서론 Introduction](#1)
2. [NRF 표현 Neural Representation](#2)
3. [볼륨 렌더링 Volume Rendering](#3)
4. [NeRF 최적화 Optimization](#4)
5. [실험 및 결과 Results](#5)
6. [어블레이션 연구 Ablation Studies](#6)
7. [한계 및 토의 Discussion](#7)
8. [개인 인사이트 Insights](#8)
9. [결론 Conclusion](#9)

---
<Image
  src="/images/paper-review/NeRF/overview-nerf.png"
  alt="NeRF Overview"
  width={800}
  height={450}
/>


&nbsp;&nbsp;최근 진행한 로봇 프로젝트에서, 로봇이 물건을 집거나 조작하는 과정에서 정확한 3차원 장면 복원 없이는 정밀한 제어가 불가능하다는 점을 실감했습니다. 단순히 대상의 위치를 아는 것을 넘어, 형상, 깊이, 반사, 투명도 등 시각적 복합성을 복원하는 능력이 필수적입니다.
&nbsp;&nbsp;이러한 문제의식에서 출발해, 저는 장면의 형태와 구조를 고정밀도로 복원하는 최신 연구들—특히 Neural Radiance Fields (NeRF)을 집중적으로 리뷰하게 되었습니다.  
단순히 논문을 읽는 것에 그치지 않고, 이를 실제 **로봇 조작 및 인지 문제에 어떻게 적용할 수 있을지** 탐구하며, 제 프로젝트를 심화시키고 있습니다.


---


## 1. Introduction


&nbsp;&nbsp;우리가 눈으로 보는 세계는 조명, 그림자, 반사, 투과 등 수많은 광학 요소가 섬세하게 상호작용하는 3차원 공간입니다.  NeRF는 어떤 3차원 위치에서 특정 방향을 바라보았을 때 나오는 색과 밀도를 예측하는 신경망 함수라고 이해하면 됩니다. 
이러한 장면을 다수의 2D 이미지로부터 복원하고, 보지 못한 시점에서 실사처럼(render photorealistic) 재구성하는 것이 Novel View Synthesis의 핵심 과제입니다.
&nbsp;&nbsp;기존 방식(복셀, 메시, 포인트 클라우드)은 고해상도 표현에 엄청난 메모리와 연산량이 요구된다는 단점이 있습니다.
NeRF는 이러한 한계를 넘어서기 위해, 장면을 5차원 함수로 표현하고 이를 MLP로 직접 학습하여 새로운 시점의 이미지를 생성합니다.

---

###  NeRF의 핵심 아이디어 정리

#### ✅ 1. 연속적인 5D 신경 복사장 (Neural Radiance Field)
기존 복셀 기반 표현의 한계를 극복하고, MLP를 통해 **기하 구조와 재질을 연속 함수로 표현**합니다.

#### ✅ 2. 미분 가능한 볼륨 렌더링 최적화
**RGB 이미지 + 카메라 포즈 정보만으로** 장면을 학습합니다.  
coarse-to-fine 구조의 **계층적 샘플링(Hierarchical Sampling)**을 통해 **정밀도와 효율성**을 동시에 달성합니다.

#### ✅ 3. 고주파 표현을 위한 Positional Encoding
MLP가 고주파 신호를 잘 학습하지 못하는 문제를 극복하기 위해, **입력 좌표를 고차원 사인/코사인 임베딩**하여 주기성을 부여하고 세밀한 표현을 가능하게 합니다.

---

NeRF는 3D 위치 벡터 $\mathbf{x} \in \mathbb{R}^3$와 시점 방향 벡터 $\mathbf{d} \in \mathbb{R}^3$를 입력받아, **색상 $\mathbf{c}$와 밀도 $\sigma$를 출력**하는 MLP를 학습합니다.

> ### 🧾 정리
> - **연속적인 MLP 기반 장면 표현**
> - **RGB와 포즈 정보만으로 학습 가능한 볼륨 렌더링**
> - **Positional Encoding을 통한 고주파 표현 학습**

---

## 2. NeRF의 동작 메커니즘: 전체 파이프라인 분석

<Image src="/images/paper-review/NeRF/pipeline.jpg" alt="NeRF pipeline" width={800} height={450} />
>  &nbsp;&nbsp;NeRF의 입력부터 출력까지의 전체 파이프라인 과정을 시각적으로 요약한 것이다.

### (a) 5D Input (Position + Direction)
&nbsp;&nbsp;카메라 픽셀을 기준으로, rays를 따라 여러 위치 $(x,y,z)$와 를 샘플링하고, 시점 방향 $(\theta,\phi)$와 결합해 **5D 좌표**로 구성합니다. 이는 MLP $F_\Theta$의 입력이 됩니다.
Ray는 카메라 위치에서 픽셀을 기준으로 뻗어나가는 광선입니다. 예를 들어 400×400 해상도의 이미지는 160,000개의 Ray를 생성하고, 학습 시에는 이 중 4,096개를 무작위로 샘플링합니다. 각 Ray는 연속적인 지점을 무한히 포함하지만, 계산 효율을 위해 각 Ray당 64~128개의 지점을 Stratified Sampling으로 선택하여 MLP에 전달합니다.
&nbsp;&nbsp;이때 Ray의 시작점과 방향은 COLMAP 등을 통해 얻어진 **카메라 extrinsic matrix (camera-to-world)**와 intrinsic parameter (초점 거리, 주점 등)를 통해 계산됩니다. 각 이미지에 대한 pose 정보는 transforms.json 형식의 파일에 저장됩니다.
### (b) Output: Color + Density
MLP는 입력에 대해 **밀도 $\sigma$**와 **색상 $\mathbf{c}=(r,g,b)$**를 예측합니다.  
- 밀도 $\sigma$: 위치 $(x,y,z)$에만 의존  
- 색상 $\mathbf{c}$: 위치 + 시점 방향 모두에 의존

→ 시점 변화에 따른 **반사, 투명도 등 비-Lambertian 특성**도 표현 가능
MLP는 먼저 위치 정보 $\gamma(x)$만을 입력으로 사용하여 밀도 $\sigma$와 중간 feature vector를 예측하고, 이 feature와 방향 정보 $\gamma(d)$를 결합해 최종 색상을 출력합니다. 이때 사용하는 $\gamma(\cdot)$는 Positional Encoding 함수로, 고주파 표현이 가능하도록 입력 좌표를 고차원 공간으로 임베딩합니다. 일반적으로 $\gamma(x)$는 60차원, $\gamma(d)$는 24차원으로 변환됩니다.

### (c) Volume Rendering
예측된 $(\sigma_i, c_i)$ 쌍은 픽셀 하나를 구성하는 데 모두 기여합니다. NeRF는 광선(r) 상의 색상 기여도를 누적하여 최종 픽셀 값을 계산하며, 아래의 볼륨 렌더링 수식을 사용합니다:
$$
C(r) = \int_{t_n}^{t_f} T(t) \cdot \sigma(t) \cdot c(t, d) \, dt
$$

이때 $T(t)$는 $t$까지의 누적 투과율로 다음과 같이 정의됩니다:

$$
T(t) = \exp \left( - \int_{t_n}^{t} \sigma(s) \, ds \right)
$$

### (d) Rendering Loss
렌더링된 색상 $\hat{C}(r)$과 ground truth 간의 $L_2$ loss를 최소화하며 MLP 파라미터 $\Theta$를 학습합니다.
$$
\mathcal{L} = \sum_r \| \hat{C}(r) - C^*(r) \|^2
$$

NeRF는 효율성과 정밀도를 높이기 위해 Hierarchical Volume Sampling 전략을 적용합니다. 먼저 Coarse Network에서 균일 샘플링을 수행하고, 밀도가 높은 영역을 중심으로 Fine Network에서 추가 샘플링을 진행함으로써 정보가 풍부한 영역에 학습 집중도를 높입니다.

---

## 3. NeRF 구현 세부 절차

####  COLMAP을 통한 카메라 파라미터 복원

- **Camera-to-World 행렬**: 카메라 좌표계 기준 위치 및 방향 → 월드 좌표계로 변환
- **Intrinsic Parameters**: focal length (fx, fy), principal point (cx, cy)
- **Sparse 3D Point Cloud**: 시각화용이며 학습에 직접 쓰이진 않음
→ 결과는 보통 transforms.json에 저장되어 NeRF 학습에 사용

---

####  Ray 정의 및 Stratified Sampling

각 이미지의 픽셀마다 다음과 같은 ray를 생성합니다:

$$
r(t) = \mathbf{o} + t ⋅ \mathbf{d}, \quad t \in [t_n, t_f]
$$

- $\mathbf{o}$: 카메라의 위치 (Camera-to-World 행렬에서 추출)
- $\mathbf{d}$: 픽셀의 방향 벡터 (intrinsic matrix로 계산 → world 좌표계로 변환)
- $t$: ray를 따라 이동하는 거리. $t_n$은 near bound, $t_f$는 far bound


- 일부 이미지 선택 → 픽셀 수천 개 무작위 선택  
- 각 픽셀에서 ray 생성

stratified sampling: 
단순한 uniform sampling은 특정 구간에 과도한 샘플이 집중되거나, 샘플이 고르게 분포하지 않는 문제가 생길 수 있다. 이를 해결하기 위해 NeRF는 Stratified Sampling 기법을 사용한다. 이 방식에서는 $[t_n,t_f]$ 구간을 균등하게 $N$개의 작은 구간으로 나누고 각 구간 내에서 무작위로 하나의 $t_i$를 선택한다. 수식으로는 다음과 같다.


$$
t_i \sim \mathcal{U} \left[ t_n + \frac{i - 1}{N}(t_f - t_n),\; t_n + \frac{i}{N}(t_f - t_n) \right]
$$
이러한 방식은 샘플이 전체 ray 상에 고르게 분포되도록 보장하면서도 일정 수준의 무작위성을 유지하여, 학습 과정에서의 분산(variance)을 효과적으로 감소시킨다.


#### Hierarchical Sampling (Coarse → Fine)

NeRF의 핵심 아이디어 중 하나는, 장면의 중요한 영역에 더 많은 샘플을 집중하는 계층적 샘플링 (Hierarchical Sampling) 방식이다. 이를 위해 두 개의 MLP가 사용되는데, 첫 번째는 coarse network로 초기 예측을 수행하며, 두 번째는 이를 기반으로 중요도가 높은 영역을 더 정밀하게 샘플링하는 fine network이다. 
먼저, coarse 네트워크는 위에서 설명한 stratified sampling으로 $N_c = 64$개의 3차원 포인트를 ray 마다 생성하고 이를 통해 밀도 $\sigma_i$ 값을 예측한다. 이 예측값을 기반으로 각 구간의 누적 투과율과 가중치를 계산하여, 중요도가 높은 구간을 식별한다. 이렇게 얻은 분포 $w_i$는 확률 밀도 함수 로 정규화 되며 이 분포에 따라 $N_f = 128$개의 추가 샘플을 뽑는다.


---


### 4. MLP 입력 준비 (Positional Encoding 전 단계)

3D 위치 와 해당 ray 방향 를 pair로 묶어 5D 벡터 (x, d) 생성

이 벡터는 다음 단계인 Positional Encoding을 통해 고주파 표현력이 강화된 후 MLP에 입력됨

🔸 최종 입력 요약

MLP 입력 전 형식: $\mathbf{(x,d)} \in \mathbb{R}^5$ (position 3D + direction unit vector)


가정 (Assumptions):

- Volume density : 위치 x에만 의존

- Color : 위치 x와 방향 d 모두에 의존

이렇게 구성된 5D 입력은 이후 NeRF의 핵심인 MLP에 전달되어 밀도와 색상을 출력합니다.



### MLP 구조
<Image src="/images/paper-review/NeRF/architecture.png" width={800} height={450} alt="..." />
Input: γ(x) → [FC×8, width=256] → skip connection at 4th → density σ + feature  
→ concat γ(d) → [FC×2, width=128] → RGB

---

### 1. Positional Encoding

$$
\gamma(p) = \left[
\sin(2^0 \pi p), \cos(2^0 \pi p), \ldots,
\sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p)
\right]
$$

- 위치 인코딩: \( L = 10 \Rightarrow 60 \)차원  
- 방향 인코딩: \( L = 4 \Rightarrow 24 \)차원  


### 2. Volume Rendering

카메라 레이:
$$
\mathbf{r}(t) = \mathbf{o} + t \mathbf{d}
$$

색상:
$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \, \sigma(t) \, c(t, \mathbf{d}) \, dt
$$

누적 투과율:
$$
T(t) = \exp\left( -\int_{t_n}^t \sigma(s) \, ds \right)
$$

### Stratified Sampling (이산 근사)

$$
C(\mathbf{r}) \approx \sum_{i=1}^{N} w_i \, c_i,\quad
w_i = T_i \left( 1 - \exp(-\sigma_i \delta_i) \right)
$$

## 3. NeRF 최적화

Loss Function:
$$
\mathcal{L} = \sum_{\mathbf{r}} \left( \| C_c(\mathbf{r}) - C^*(\mathbf{r}) \|_2^2 + \| C_f(\mathbf{r}) - C^*(\mathbf{r}) \|_2^2 \right)
$$

### Hierarchical Sampling

- Coarse: \( N_c = 64 \)
- Fine: \( N_f = 128 \) → 총 192개 샘플링  
- 중요도 샘플링: Coarse 결과의 weight \( w_i \)를 기반으로 PDF 재샘플링  

## 4. 실험 및 결과

### Datasets

- Synthetic 360° (Lambertian)
- Realistic Synthetic 360° (Non-Lambertian)
- LLFF Forward-Facing (실제 장면)

### Metrics

| Metric | 의미               |
|--------|--------------------|
| PSNR   | 재구성 품질 (높을수록 좋음) |
| SSIM   | 구조 유사도        |
| LPIPS  | 시각적 유사도      |

## 5. 어블레이션 연구

| 구성                     | PSNR   |
|--------------------------|--------|
| Base MLP                 | 25.80  |
| + Positional Encoding    | 30.31  |
| + View-dependent color   | 31.37  |
| + Hierarchical Sampling  | 34.65  |


## 6. 한계 및 토의

| 장점                    | 한계                          |
|-------------------------|-------------------------------|
| 고해상도 표현 가능       | 학습 시간 수 시간 이상 소요    |
| 메모리 효율적 (약 5MB)  | Scene별 재학습 필요           |

### 후속 연구

- **Instant-NGP**: 수분 내 학습
- **NeRF-W**: 노이즈 및 조도 보정
- **Dynamic NeRF**: 시간축 추가

## 7. 개인 인사이트

- NeRF는 **고정된 장면(Scene-specific)**이란 특성 때문에 일반화는 어려우나, **고품질 정적 표현**에서는 독보적인 성능을 보임
- **Positional Encoding 구조 덕분에 고주파 표현력이 뛰어남**

## 8. 결론

NeRF는 복잡한 기하와 재질을 하나의 MLP로 연속적으로 표현하는 breakthrough였으며, 이후 수많은 확장 논문들이 등장하며 딥러닝 기반 3D 장면 표현의 새로운 기준이 되었다.

## 참고자료

- [Original NeRF Paper](https://arxiv.org/abs/2003.08934)
- [Official Code (GitHub)](https://github.com/bmild/nerf)
- [Instant-NGP (2022)](https://github.com/NVlabs/instant-ngp)
- [NeRF-W (2021)](https://arxiv.org/abs/2012.09730)