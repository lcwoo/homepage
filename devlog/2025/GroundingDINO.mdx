---
title: "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"
date: "2025-05-17"
summary: "paper review"
---

> 2023년 발표된 Grounding DINO 논문   
> [[paper]](https://arxiv.org/abs/2303.05499), [[GitHub]](https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file)  
> Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang  
> BNRist Center, IDEA, HKUST, The Chinese University of Hong Kong, Microsoft Research   
> ECCV 2024  
> 2023년 3월 9일  

## 목차 / Table of Contents

1. 서론 (Introduction)  
2. 관련 연구 (Related Works)  
3. 방법론 (Method)
4. 손실 함수 (Loss Function)  
5. 실험 및 결과 (Experiments and Results)  
6. 한계점 (Limitations) 
---

<Image
  src="/images/paper-review/GroundingDINO/groundingdino.png"
  alt="GroundingDINO overview"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

최근 멀티모달 인식과 open-vocabulary 학습에 관심을 가지며 CLIP 논문을 리뷰하게 되었습니다. 
CLIP은 다양한 비전-언어 모델의 기반이 되었으며 특히 zero-shot classification에서 뛰어난 성능을 보여주었습니다. 
하지만 단순한 임베딩 정렬 방식만으로는 실제 환경에서 요구되는 open-vocabulary object detection, 언어 기반 인지, 
multi-modal reasoning까지 확장하기에 구조적인 한계가 있다고 생각했습니다.  
텍스트 조건에 따라 객체를 탐지하고 시각·언어 정보를 모델 내부에서 깊이 있게 융합하는 연구를 찾으면서 Grounding DINO를 알게되었습니다. 
Transformer 기반 DINO를 바탕으로 텍스트 정보를 모델 전반에 attention을 활용하면서 통합함으로써 open-set object detection을 수행합니다.

## 1. 서론 (Introduction)

최근의 object detection 기술은 대부분은 미리 정의된 label에 한해 탐지가 가능한 close set을 기반으로 하고 있습니다. 
학습되지 않은 새로운 객체나 개념에 대해서는 대응할 수 없는 구조적 한계를 갖고 있습니다. 
이러한 한계를 극복하기 위해 연구자들은 open-set object detection 문제에 주목하고 있습니다. 
Grounding DINO는 자연어와 이미지 정보를 통합 처리하는 Transformer 기반의 구조적 강점을 적극 활용합니다. 
단순히 텍스트와 시각 정보를 후처리 단계에서 결합하는 것이 아니라 
모델의 인코더와 디코더 내부에서 다단계 cross-attention을 통해 양 modality의 정보를 정밀하게 정렬합니다. 
이로 인해 텍스트와 시각 정보 간의 깊이 있는 의미적 대응이 가능해지며 보다 정밀하고 유연한 객체 탐지 결과를 얻을 수 있습니다.  

---

## 2. 관련 연구 (Related Works)

ViLD와 RegionCLIP은 사전학습된 CLIP 모델이 지닌 광범위한 언어 지식을 활용해 이미지 영역을 텍스트 임베딩과 정렬하는 방식을 사용했습니다. 
이들은 기존 탐지기의 시각 표현을 텍스트 공간으로 투영하는 지식 증류 방식을 통해 CLIP의 open vocabulary 능력을 간단히 전이할 수 있었습니다. 
그러나 구조적으로 이미지와 텍스트를 별도로 인코딩하고 마지막에만 비교하는 two-tower 구조였기 때문에 시각-언어 간 깊은 상호작용에는 한계가 있었습니다. 
MDETR은 문장 단위의 명시적인 grounding supervision을 도입해 멀티모달 정렬을 보다 직접적으로 수행하려는 시도를 했습니다. 
Transformer 구조를 바탕으로 시각과 언어 정보를 통합했지만 여전히 모델 내부에서 시각-언어 간 정보를 충분히 공유하지 못하는 구조적 한계가 존재했습니다.  
이러한 문제를 해결하고자 등장한 GLIP는 객체 탐지 문제를 문장-영역 매칭 문제로 재정의했습니다. 
이미지와 문장 쌍을 입력으로 받아 각 텍스트 프레이즈가 가리키는 시각적 영역을 찾도록 훈련되었으며 
contrastive learning을 통해 텍스트-이미지 영역 간 임베딩을 정렬했습니다. 
특히 COCO, Objects365, Visual Genome 등 이질적인 데이터셋을 통합하여 폐쇄형 및 개방형 탐지에서 모두 뛰어난 성능을 기록했습니다.  
하지만 GLIP는 여전히 CNN 기반의 one-stage Dynamic Head를 사용하여 
Transformer 구조에 비해 언어 정보를 계층 전체에 걸쳐 활용하기 어려웠고 
실제로도 초기 백본 출력을 제외하고는 언어 정보가 활용되지 않았습니다.

#### <HighlightText lightColor="red.300" darkColor="#ffb163" fontSize="xl"> Grounding DINO의 Contributions</HighlightText>

<Image
  src="/images/paper-review/GroundingDINO/extending_closedset.png"
  alt="GroundingDINO"
  width={600}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>


Grounding DINO는 위의 한계들을 해결하고자 Transformer 기반의 객체 탐지 모델인 DINO를 바탕으로 
모델 전반에 걸친 시각-언어 통합 구조를 설계한 것이 핵심 기여입니다. 
DINO는 Deformable DETR의 계열로서 다중 해상도의 특성 맵을 처리할 수 있는 Deformable Attention 구조를 갖추고 있으며 
contrastive denoising과 mixed query selection 등 학습 안정화 기법을 통해 COCO 데이터셋에서 SOTA 성능을 기록한 바 있습니다.  
Grounding DINO는 이러한 DINO의 구조적 장점을 바탕으로 언어 정보를 3단계에 걸쳐 통합합니다.  
- <HighlightText lightColor="cyan.700" darkColor="#8be9fd"> feature enhancer 단계에서는 백본에서 추출된 시각 피처와 텍스트 임베딩을 결합하여 멀티모달 표현을 강화합니다.  </HighlightText>
- <HighlightText lightColor="cyan.700" darkColor="#8be9fd"> query selection 단계에서는 자연어 정보를 반영하여 의미 있는 디코더 쿼리를 선택합니다.   </HighlightText>
- <HighlightText lightColor="cyan.700" darkColor="#8be9fd"> 디코더 내부 매 레이어마다 self-attention과 cross-attention을 통해 시각과 언어 표현이 지속적으로 상호작용하게 구성됩니다.   </HighlightText>

---

## 3. 방법론 (Method)

<Image
  src="/images/paper-review/GroundingDINO/framework_groundingdino.png"
  alt=" The framework of Grounding DINO"
  width={600}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>


#### Feature Extraction and Enhancer

Grounding DINO는 Dual-Encoder 구조를 통해 입력 이미지와 텍스트를 독립적으로 인코딩합니다. 
이미지 인코더로는 주로 Swin Transformer를 텍스트 인코더로는 BERT 계열 모델을 사용하여 
각각 멀티스케일 이미지 특징 맵과 단어 단위 텍스트 임베딩을 출력합니다. 
특히 텍스트 표현은 단어 수준 혹은 문장 수준 표현 대신 sub-sentence 수준 표현을 도입하여 여러 명사구가 하나의 문장으로 들어왔을 때 
서로 불필요한 attention을 주고받지 않도록 attention mask를 설정합니다. 
이를 통해 의미적 간섭 없이 다수의 텍스트 표현을 동시에 처리할 수 있습니다. 
인코딩된 이미지,텍스트 특징은 Feature Enhancer로 전달되며 이 모듈은 여러 개의 layer로 구성된 cross-modal feature fusion 블록입니다. 

<Image
  src="/images/paper-review/GroundingDINO/enhancer_layer.png"
  alt="GroundingDINO"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

<HighlightText lightColor="red.300" darkColor="#ffb163"> 각 layer는 이미지와 텍스트 각각에 대한 self-attention을 수행한 후 텍스트→이미지와 이미지→텍스트 방향의 cross-attention을 순차적으로 적용합니다. </HighlightText>
이미지 self-attention은 Deformable Attention 방식을 사용해 각 위치에서 제한된 영역만 참조하면서도 
전역 정보를 효율적으로 수용하고 텍스트 self-attention은 문맥 정보를 정제하는 역할을 합니다. 
이어지는 cross-attention 단계에서는 각 텍스트 토큰이 이미지에서 의미적으로 관련된 영역의 정보를 받아 표현이 강화되고 
반대로 각 이미지 위치는 자신과 가장 관련된 텍스트 표현을 받아 언어적으로 정렬된 표현으로 변환됩니다. 
이 과정을 여러 번 반복함으로써 텍스트에서 언급된 객체는 이미지 특징 내에서 더욱 두드러지게 되고 배경은 억제되는 방향으로 표현이 정제됩니다.

---

#### Language-Guided Query Selection

<Image
  src="/images/paper-review/GroundingDINO/Pseudocode_of_Language-guided_Query_Selection.png"
  alt="GroundingDINO"
  width={550}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

Grounding DINO는 기존 close-set 객체 탐지기처럼 고정된 클래스 집합을 대상으로 모든 클래스를 탐지하도록 학습하는 방식과 다릅니다. 
일반적인 DETR 계열 모델에서는 디코더 쿼리를 학습 가능한 정적 벡터로 초기화하여 입력 이미지 전체를 대상으로 모든 클래스에 대한 탐지를 시도합니다. 
Grounding DINO는 텍스트 정보를 기반으로 디코더 쿼리를 동적으로 선택함으로써 입력된 언어 조건에 따라 어떤 영역을 탐지할지를 조정하는 구조를 갖습니다.
<HighlightText lightColor="red.300" darkColor="#ffb163">텍스트와 이미지 간의 의미 정렬을 기반으로 쿼리를 선택하는 Language-Guided Query Selection 모듈을 사용합니다.</HighlightText>
이미지 백본으로부터 얻은 $M$개의 이미지 위치 특징과 텍스트 인코더(BERT 등)에서 추출한 $N$개의 텍스트 토큰 임베딩 간의 유사도를 계산합니다. 
유사도는 dot product를 통해 유사도를 구한 뒤 
그 중 가장 텍스트와 관련이 높은 상위 $K$개의 이미지 토큰을 선택합니다. 
선택된 이미지 위치는 이후 anchor box 기반의 positional embedding으로 변환되어 쿼리의 위치 정보를 초기화합니다. 
이는 단순히 의미적 유사성만 고려하는 것이 아니라 어떤 이미지 공간 위치에 attention을 집중해야 하는지를 명시적으로 반영합니다. 
반면 content embedding은 DINO의 Mixed Query Selection 전략을 따라 학습 가능한 쿼리 벡터로 설정되어 일정 수준의 일반성과 적응성을 함께 확보합니다. 
query selection이 이루어진 쿼리는 이후 Cross-Modality Decoder로 전달되어 이미지와 텍스트 정보를 바탕으로 반복적으로 refinement되며 객체 예측을 수행합니다.

---

#### Cross-Modality Decoder

<Image
  src="/images/paper-review/GroundingDINO/Cross_Modality_Decoder.png"
  alt="GroundingDINO"
  width={550}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

선택된 쿼리들은 Cross-Modality Decoder에 입력되어 반복적인 업데이트를 통해 최종 객체 예측 결과를 생성합니다. 
이 디코더는 Grounding DINO에서 핵심적인 역할을 수행하며 기존 DINO의 디코더 구조를 기반으로 하되 
텍스트 정보를 효과적으로 활용하기 위해 텍스트 cross-attention 블록이 추가된 점이 가장 큰 차별점입니다. 
디코더는 총 6개의 동일한 구조의 레이어로 구성되어 있으며 
각 레이어는 순차적으로 self-attention, 이미지 cross-attention, 텍스트 cross-attention, feed-forward network 연산을 수행합니다. 

> (A.1을 참고)  

쿼리 간 self-attention은 객체 간 문맥적 상호작용을 통해 표현을 조정하며 deformable 이미지 cross-attention은 
anchor 주변의 이미지 영역만을 선택적으로 참조하여 계산 효율성과 정밀한 위치 정보를 동시에 확보합니다. 
이어지는 텍스트 cross-attention은 쿼리가 자신과 가장 관련 있는 텍스트 토큰으로부터 의미 정보를 받아 표현을 보강하는 역할을 합니다. 
이 과정을 통해 각 쿼리는 반복적인 refinement를 거쳐 시각적 위치 정보와 언어적 의미 정보를 동시에 통합한 객체 표현으로 점차 발전하게 됩니다.
최종 디코더 출력은 각 쿼리에 대해 bounding box 좌표 $(x,y,w,h)$와 해당 객체가 어떤 텍스트 토큰과 
가장 관련 있는지를 나타내는 matching score로 구성됩니다. 
텍스트가 클래스 리스트일 경우에는 multi-class classification 점수로 단일 지시문일 경우에는 그 문장 전체와의 matching score로 해석되니다. 
특히 referring expression comprehension(REC) 태스크에서는 출력 쿼리들 중 텍스트와 가장 높은 일치도를 보이는 하나의 박스만을 예측 결과로 사용하게 됩니다.

---

#### Sub-Sentence Level Text Feature

<Image
  src="/images/paper-review/GroundingDINO/Sub_sentence_level.png"
  alt="Grounding DINO"
  width={550}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

Grounding DINO는 텍스트 인코딩 시 기존 word-level 혹은 sentence-level 표현이 갖는 문제점을 해결하기 위해 sub-sentence 수준 표현을 도입하였습니다. 
단어 수준 표현은 여러 클래스 이름을 나열할 때 서로 관계없는 단어들 간에도 어텐션이 발생하는 문제를 갖고 있으며 
문장 수준 표현은 문장 전체를 하나의 벡터로 압축하면서 세부 정보를 손실할 수 있습니다. 
이에 대해 Grounding DINO는 마침표 등의 구분자를 사용해 입력 텍스트를 여러 서브 문장 단위로 나누고 
각 서브 문장 내에서만 self-attention이 작동하도록 attention mask를 구성합니다. 
이를 통해 각 카테고리 표현은 독립적으로 인코딩되며 서로 다른 클래스 이름 간의 간섭을 방지할 수 있습니다. 
COCO 클래스 이름 80개를 나열할 때 "person. bicycle. car. ..." 식으로 입력하면 각각 독립적으로 처리되면서도 병렬적으로 인코딩될 수 있습니다.  

---

## 4. 손실 함수 (Loss Function)

Grounding DINO는 객체의 위치와 클래스 예측뿐만 아니라, 이미지와 텍스트 간의 의미 정렬까지 동시에 고려해야 하는 멀티모달 오픈어휘 객체 탐지기입니다. 
따라서 단순한 박스 회귀 손실만이 아니라, 다양한 손실 항들을 조합한 복합 손실 함수를 사용합니다. 
기본적인 틀은 DETR 계열처럼 Hungarian Matching 기반의 set prediction이며 
예측 쿼리와 Ground Truth 객체 간 일대일 대응을 먼저 수행한 후 각 쌍에 대해 손실을 계산합니다.  
Grounding DINO의 전체 손실은 다음과 같은 네 가지 주요 항으로 구성됩니다:
Localization Loss는 예측된 박스 $\hat{b}$와 Ground Truth 박스 $b$ 사이의 위치 정확도를 평가하기 위해 L1 손실과 GIoU(Generalized IoU) 손실을 결합합니다

$$
\mathcal{L}_{\text{loc}} = \lambda_1 \cdot \mathcal{L}_{\text{L1}}(b, \hat{b}) + \lambda_2 \cdot \mathcal{L}_{\text{GIoU}}(b, \hat{b})
$$


Matching Classification Loss는 예측된 쿼리가 어떤 클래스에 대응되는지를 예측하도록 학습시키는 손실입니다. 
클래스 리스트가 주어졌다면 일반적인 Cross Entropy Loss를 사용하며 단일 문장이 주어졌을 경우 전체 문장과의 matching score로 해석됩니다.

$$
\mathcal{L}_{\text{cls}} = \text{CrossEntropy}(y, \hat{y})
$$



 Contrastive Alignment Loss는 이미지 토큰 $f_i$와 텍스트 토큰 $t_i$ 사이의 cosine similarity를 기반으로 멀티모달 정렬을 유도하는 손실입니다. 
양의 쌍은 더 가깝게, 음의 쌍은 멀어지도록 학습시킵니다.

$$
\mathcal{L}_{\text{contrast}} = -\log \frac{\exp(\text{sim}(f_i, t_i)/\tau)}{\sum_j \exp(\text{sim}(f_i, t_j)/\tau)}
$$

 No-object Focal Loss는 매칭되지 않은 쿼리들에 대해 "no object" 클래스를 예측하도록 유도하는 손실입니다. 
이 항은 false positive를 줄이고 모델이 의미 없는 위치에 주의를 분산하지 않도록 학습을 유도합니다.

$$
\mathcal{L}_{\text{no\_obj}} = \text{FocalLoss}(y_{\text{null}}, \hat{y})
$$

위 모든 손실 항들을 합산하여 Grounding DINO의 최종 손실 함수는 다음과 같이 표현됩니다.

$$
\mathcal{L}_{\text{total}} = \lambda_1 \cdot \mathcal{L}_{\text{L1}} + \lambda_2 \cdot \mathcal{L}_{\text{GIoU}} 
+ \lambda_3 \cdot \mathcal{L}_{\text{cls}} + \lambda_4 \cdot \mathcal{L}_{\text{contrast}} + \lambda_5 \cdot \mathcal{L}_{\text{no\_obj}}
$$

$\lambda_1 = 5.0$ (L1), $\lambda_2 = 2.0$ (GIoU), $\lambda_3 = 1.0$ (Classification)

---

## 5. 실험 및 결과 (Experiments and Results)  

<Image
  src="/images/paper-review/GroundingDINO/zeroshot_coco.png"
  alt="Grounding DINO"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

Grounding DINO는 COCO와 LVIS에서 open-set object detection 성능을 중심으로 평가되었습니다. 
특히 학습에 포함되지 않은 novel class에 대한 zero-shot detection 성능과 fine-tuning 이후 성능을 함께 비교하였습니다.
Grounding DINO는 COCO 데이터셋에서 pretraining만으로 COCO fine-tuning 없이도 49.2 AP라는 높은 정확도를 달성했습니다. 
기존 SOTA 기법들을 뛰어넘는 수치입니다.  
클래스 수가 훨씬 많은 LVIS 데이터셋에서도 강력한 성능을 보였습니다. 
Swin-T 백본을 사용할 경우 zero-shot LVIS에서 25.5 AP를 기록하며 open-vocabulary 환경에서도 robustness를 입증했습니다.

Grounding DINO의 성능은 대표적인 객체 탐지 지표인 Average Precision(AP)을 기준으로 측정됩니다. 
AP는 예측한 박스가 실제 객체와 얼마나 잘 일치하는지를 나타내는 정밀도-재현율 곡선의 면적(AUC)으로,
IoU(Intersection over Union) 기준을 다양하게 적용해 평균화한 지표입니다.   
세부지표는 LVIS 데이터셋의 클래스 불균형을 반영하여 클래스 등장 빈도에 따라 성능을 분석하는 방식입니다.  
- AP<sub>r</sub> (Rare): 학습 데이터에서 희귀하게 등장한 클래스에 대한 평균 정밀도
- AP<sub>c</sub> (Common): 중간 정도로 등장한 클래스에 대한 평균 정밀도
- AP<sub>f</sub> (Frequent): 자주 등장하는 빈번 클래스에 대한 평균 정밀도


<Image
  src="/images/paper-review/GroundingDINO/ablation.png"
  alt="Grounding DINO"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

Swin-T 백본과 O365 pretrain 기반 모델을 바탕으로 진행되었습니다.  
Feature Enhancer를 제거한 모델(1번)과 이를 포함한 모델(0번 Full model)을 비교하면, 
encoder fusion이 COCO와 LVIS 모두에서 성능을 유의하게 향상시킴을 확인할 수 있습니다. 
cross-modality 정보 융합이 단순 텍스트 후처리보다 훨씬 효과적이라는 것을 알 수 있습니다.
Language-Guided Query Selection은 LVIS에서 +3.0 AP의 향상을 했고 COCO의 zero-shot 성능도 향상시켰습니다.  
Query 선택에서 텍스트 정보를 활용하는 것이 특히 클래스 수가 많은 open-vocabulary 환경에서 유용하다는 것을 알 수 있습니다.  
Text Cross-Attention은 비교적 적은 수의 파라미터 증가에도 불구하고 LVIS에서 +1.8 AP의 개선을 달성했고 
COCO zero-shot에서도 기능을 했습니다. 
Sub-Sentence Level Text Prompt는 LVIS에서 +0.5 AP 향상을 보이며 문장 내 불필요한 단어 간 
상호작용을 제거하는 것이 효과적임을 보여주었습니다.  
하지만 zero-shot 성능 향상에는 긍정적인 영향을 미치지만 COCO fine-tune 상황에서는 상대적으로 영향이 작았습니다. 
모델 파라미터를 직접적으로 수정하거나 계산량을 늘리지 않기 때문에 fine-tuning 성능에는 제한적인 영향을 준다는 것을 알 수 있습니다. 
반면 encoder fusion은 파라미터 증가를 수반하므로 fine-tuning 환경에서도 꾸준한 성능 향상을 보였습니다. 



## 6. 한계점 (Limitations)

- Segmentation 불가

Grounding DINO는 bounding box 기반 탐지만 지원

- False Positive 문제 (Hallucination)

일부 상황에서 텍스트 조건과 무관한 객체를 탐지.

- Fine-tuning Trade-off

COCO로 fine-tuning 시 COCO 성능은 향상되나 LVIS 및 ODinW의 성능은 감소 (generalization 문제)

- 높은 연산 자원 요구

Swin-L 백본 + multi-head attention 구조를 사용해 학습 및 추론 모두 계산량 큼 → 실시간 응용에 부적합


